{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8550ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell - check correctness values in data\n",
    "import torch\n",
    "sample_batch = next(iter(DataLoader(DualEncoderDataset(train_data, Config.MAX_SEQ_LEN), batch_size=1)))\n",
    "print(\"Sample batch correctness values:\")\n",
    "print(sample_batch['correctness'])\n",
    "print(f\"\\nMin: {sample_batch['correctness'].min()}, Max: {sample_batch['correctness'].max()}\")\n",
    "print(f\"Unique values: {torch.unique(sample_batch['correctness'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7427525",
   "metadata": {},
   "source": [
    "# üéØ Knowledge Tracing with Dual Encoder + LLM Pipeline\n",
    "\n",
    "## Objective\n",
    "Predict whether a student will answer a question **Correctly** or **Incorrectly** by combining:\n",
    "1. **Context Encoder** - Text embeddings of question content and concept descriptions (supports Chinese)\n",
    "2. **Sequence Encoder** - Student learning history embeddings (question/concept sequences)\n",
    "3. **LLM** - Fine-tuned with LoRA for final prediction\n",
    "\n",
    "## Architecture: Dual Encoder System\n",
    "```\n",
    "Question_i = ContextEncoder(question_text) + SequenceEncoder(question_sequence)\n",
    "Concept_j = ContextEncoder(concept_text) + SequenceEncoder(concept_sequence)\n",
    "```\n",
    "\n",
    "## Pipeline Overview\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1. Setup | Load config, select preset, initialize environment |\n",
    "| 2. EDA | Load MOOCRadar dataset, build concept/question mappings |\n",
    "| 3. Data Prep | Create data loaders with concept mapping (Chinese ‚Üí IDs) |\n",
    "| 4. Embedding Model | Initialize dual encoder (Context + Sequence) |\n",
    "| 5. LLM Fine-tuning | Train KnowledgeTracingLLM with embedded prompts |\n",
    "| 6. Evaluation | Test predictions and compute metrics |\n",
    "\n",
    "## Model Presets\n",
    "- **small**: TinyLlama-1.1B, minimal data, for quick testing\n",
    "- **standard**: Phi-2, balanced performance  \n",
    "- **phi3**: Phi-3 Mini, excellent quality\n",
    "- **qwen**: Qwen2-1.5B, excellent for Chinese content\n",
    "- **llama2**: Llama-2-7B, highest quality (needs more GPU)\n",
    "\n",
    "## New Modular Architecture\n",
    "- `models/encoders.py` - ContextEncoder, SequenceEncoder\n",
    "- `models/embedding_model.py` - KTEmbeddingModel\n",
    "- `models/kt_llm.py` - KnowledgeTracingLLM with prompt formatting\n",
    "- `utils/concept_mapping.py` - ConceptMapper, QuestionMapper\n",
    "- `utils/data_loader.py` - Data loading utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a82f869b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Proxy settings loaded from .env file\n",
      "  HTTP Proxy: http://10.61.11.42:3128\n",
      "‚úì Hugging Face token loaded from .env file\n",
      "‚úì Applied preset: Small (Testing)\n",
      "  Minimal configuration for quick testing and debugging\n",
      "  LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Embedding dim: 768 ‚Üí LLM hidden: 2048\n",
      "‚úì Applied preset: Small (Testing)\n",
      "  Minimal configuration for quick testing and debugging\n",
      "  LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  Embedding dim: 768 ‚Üí LLM hidden: 2048\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "AVAILABLE MODEL PRESETS\n",
      "======================================================================\n",
      "\n",
      "üì¶ small ‚Üê ACTIVE\n",
      "   Name: Small (Testing)\n",
      "   Description: Minimal configuration for quick testing and debugging\n",
      "   LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "   Hidden Size: 2048\n",
      "   Data: 5 users, 100 interactions\n",
      "\n",
      "üì¶ standard\n",
      "   Name: Standard (Balanced)\n",
      "   Description: Balanced configuration for good results with reasonable resources\n",
      "   LLM: microsoft/phi-2\n",
      "   Hidden Size: 2560\n",
      "   Data: 100 users, 5000 interactions\n",
      "\n",
      "üì¶ phi3\n",
      "   Name: Phi-3 Mini\n",
      "   Description: Microsoft Phi-3 Mini - excellent performance for its size\n",
      "   LLM: microsoft/Phi-3-mini-4k-instruct\n",
      "   Hidden Size: 3072\n",
      "   Data: 200 users, 10000 interactions\n",
      "\n",
      "üì¶ qwen\n",
      "   Name: Qwen2-1.5B\n",
      "   Description: Alibaba Qwen2 - excellent for Chinese content\n",
      "   LLM: Qwen/Qwen2-1.5B\n",
      "   Hidden Size: 1536\n",
      "   Data: 200 users, 10000 interactions\n",
      "\n",
      "üì¶ llama2\n",
      "   Name: Llama-2-7B\n",
      "   Description: Meta Llama 2 - powerful but requires more resources\n",
      "   LLM: meta-llama/Llama-2-7b-hf\n",
      "   Hidden Size: 4096\n",
      "   Data: 500 users, 20000 interactions\n",
      "\n",
      "======================================================================\n",
      "Usage: Config.use_preset('preset_name')\n",
      "======================================================================\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "\n",
      "üéØ ACTIVE PRESET: SMALL\n",
      "   Small (Testing) - Minimal configuration for quick testing and debugging\n",
      "\n",
      "üìä DATA:\n",
      "  Max Interactions: 100\n",
      "  Max Users: 5\n",
      "  Max Sequence Length: 50\n",
      "  Test Split: 20%\n",
      "\n",
      "üß† AKT MODEL:\n",
      "  Embedding Dim: 768\n",
      "  Attention Heads: 4\n",
      "  Transformer Layers: 2\n",
      "  Batch Size: 32\n",
      "  Epochs: 5\n",
      "  Learning Rate: 0.0001\n",
      "  Dropout: 0.15\n",
      "\n",
      "ü§ñ LLM FINE-TUNING:\n",
      "  Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  LLM Hidden Size: 2048\n",
      "  Soft Tokens: 4\n",
      "  Batch Size: 8\n",
      "  Epochs: 2\n",
      "  Learning Rate: 0.0002\n",
      "\n",
      "üîó EMBEDDING ADAPTER:\n",
      "  AKT ‚Üí LLM: 768 ‚Üí 2048\n",
      "  Adapter Hidden: 1408\n",
      "  Soft Tokens: 4\n",
      "\n",
      "üîß LORA:\n",
      "  Rank: 8\n",
      "  Alpha: 16\n",
      "  Dropout: 0.1\n",
      "\n",
      "‚öôÔ∏è  OPTIMIZATION:\n",
      "  GPU: ‚úì Enabled\n",
      "  Mixed Precision: ‚úì Enabled\n",
      "  Gradient Accumulation: 2 steps\n",
      "\n",
      "üìä EXPERIMENT TRACKING:\n",
      "  Wandb: ‚úì Enabled\n",
      "  Entity: letrongducanh456-viettel\n",
      "  Project: LLM-KT\n",
      "  Save Model: ‚úì Yes\n",
      "\n",
      "üìÅ OUTPUT PATHS:\n",
      "  AKT Checkpoints: checkpoints/akt/small\n",
      "  LLM Checkpoints: checkpoints/llm/small\n",
      "  Embeddings: embeddings/small/\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "CREATING DIRECTORY STRUCTURE\n",
      "======================================================================\n",
      "  ‚úì AKT Model Checkpoints          ‚Üí checkpoints/akt/small\n",
      "  ‚úì LLM Model Checkpoints          ‚Üí checkpoints/llm/small\n",
      "  ‚úì General Checkpoints            ‚Üí checkpoints/small\n",
      "  ‚úì Student Embeddings             ‚Üí embeddings/small\n",
      "  ‚úì Training Outputs & Plots       ‚Üí outputs/small\n",
      "  ‚úì Training Logs                  ‚Üí logs\n",
      "  ‚úì Dataset Files                  ‚Üí dataset/MOOCRadar\n",
      "  ‚úì Concept/Question Mappings      ‚Üí mappings/small\n",
      "  ‚úì Visualizations & Assets        ‚Üí assets\n",
      "  ‚úì Embedding Cache                ‚Üí cache/embeddings\n",
      "======================================================================\n",
      "‚úì All directories created successfully\n",
      "======================================================================\n",
      "\n",
      "‚úì Using device: CPU\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: SETUP - Load Configuration & Initialize Environment\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables (includes proxy settings, WandB API key, and HF token)\n",
    "load_dotenv()\n",
    "\n",
    "# Set proxy environment variables for network requests\n",
    "if os.getenv('HTTP_PROXY'):\n",
    "    os.environ['HTTP_PROXY'] = os.getenv('HTTP_PROXY')\n",
    "    os.environ['HTTPS_PROXY'] = os.getenv('HTTPS_PROXY')\n",
    "    os.environ['http_proxy'] = os.getenv('http_proxy')\n",
    "    os.environ['https_proxy'] = os.getenv('https_proxy')\n",
    "    print(\"‚úì Proxy settings loaded from .env file\")\n",
    "    print(f\"  HTTP Proxy: {os.environ['HTTP_PROXY']}\")\n",
    "\n",
    "# Set Hugging Face token for model downloads\n",
    "if os.getenv('HF_TOKEN'):\n",
    "    os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "    os.environ['HUGGING_FACE_HUB_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "    print(\"‚úì Hugging Face token loaded from .env file\")\n",
    "\n",
    "# Load configuration (with reload to get latest changes)\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import Config\n",
    "\n",
    "# ============================================================================\n",
    "# SELECT YOUR PRESET HERE\n",
    "# ============================================================================\n",
    "# Options: \"small\" (testing), \"standard\" (balanced), \"phi3\", \"qwen\" (Chinese), \"llama2\"\n",
    "\n",
    "PRESET = \"small\"  # ‚Üê Change this to switch models\n",
    "\n",
    "# Apply the preset\n",
    "Config.use_preset(PRESET)\n",
    "\n",
    "# Show available presets\n",
    "print(\"\\n\")\n",
    "Config.list_presets()\n",
    "\n",
    "# Show full configuration\n",
    "print(\"\\n\")\n",
    "Config.print_config()\n",
    "\n",
    "# ============================================================================\n",
    "# CREATE ALL NECESSARY DIRECTORIES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CREATING DIRECTORY STRUCTURE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "directories_to_create = [\n",
    "    (Config.AKT_CHECKPOINT_DIR, \"AKT Model Checkpoints\"),\n",
    "    (Config.LLM_CHECKPOINT_DIR, \"LLM Model Checkpoints\"),\n",
    "    (Config.CHECKPOINT_DIR, \"General Checkpoints\"),\n",
    "    (Config.EMBEDDING_DIR, \"Student Embeddings\"),\n",
    "    (Config.OUTPUT_DIR, \"Training Outputs & Plots\"),\n",
    "    (Config.AKT_LOG_DIR, \"Training Logs\"),\n",
    "    (Config.DATA_DIR, \"Dataset Files\"),\n",
    "    (Config.MAPPINGS_DIR, \"Concept/Question Mappings\"),\n",
    "    ('assets', \"Visualizations & Assets\"),\n",
    "    ('cache/embeddings', \"Embedding Cache\")\n",
    "]\n",
    "\n",
    "for dir_path, description in directories_to_create:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"  ‚úì {description:30} ‚Üí {dir_path}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úì All directories created successfully\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úì Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4fb61",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset & Build Concept Mappings\n",
    "\n",
    "In this section we:\n",
    "1. Download/cache MOOCRadar dataset from Google Drive\n",
    "2. Load problems and student interactions\n",
    "3. **Build concept mappings** (Chinese concept names ‚Üí numeric IDs)\n",
    "4. Explore data distributions\n",
    "5. Prepare data loaders using the new modular utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb0dc5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING MOOCRADAR DATASET\n",
      "======================================================================\n",
      "\n",
      "[2.0] Checking dataset files...\n",
      "‚úì Using cached file: dataset/MOOCRadar/problem.json\n",
      "‚úì Using cached file: dataset/MOOCRadar/student-problem-coarse-flattened.json\n",
      "\n",
      "[2.1] Building concept and question mappings...\n",
      "  ‚Üí Loaded question mapping (9383 questions) from mappings/small\\question_mapping.pkl\n",
      "  ‚Üí Loaded concept mapping (9985 concepts) from mappings/small\\concept_mapping.pkl\n",
      "\n",
      "‚úì Question Mapper: 9383 questions\n",
      "‚úì Concept Mapper: 9985 concepts\n",
      "\n",
      "Sample Concept Mappings (Chinese ‚Üí ID):\n",
      "  ‚Ä¢ 'ÂèçÈ¶à' ‚Üí CID=8094 (count: 133)\n",
      "  ‚Ä¢ 'ÁîµÂ≠êÁßëÂ≠¶‰∏éÊäÄÊúØ' ‚Üí CID=7898 (count: 112)\n",
      "  ‚Ä¢ 'ÊîæÂ§ßÁîµË∑Ø' ‚Üí CID=7909 (count: 84)\n",
      "  ‚Ä¢ 'Êï∞ÊçÆÊåñÊéò' ‚Üí CID=6961 (count: 70)\n",
      "  ‚Ä¢ 'Êú∫Âô®Â≠¶‰π†' ‚Üí CID=6803 (count: 69)\n",
      "\n",
      "[2.2] Loading problem database...\n",
      "  ‚Üí Loading problems from dataset/MOOCRadar/problem.json...\n",
      "  ‚Üí Loaded 9383 problems\n",
      "\n",
      "Sample problem structure:\n",
      "  ‚Ä¢ problem_id: Pm_2046133\n",
      "  ‚Ä¢ concepts: ['ÁªèÊµéÂèëÂ±ïÈÄªËæë', 'ÈÄªËæëÂ≠¶']...\n",
      "  ‚Ä¢ text (truncated): {'problem_id': 2046133, 'title': 'Á¨¨‰∏ÄÁ´†‰Ωú‰∏ö', 'content': '‚ÄúÁªèÊµéÂèëÂ±ïÊúâÂÖ∂ÂÜÖÂú®ÁöÑÈÄªËæëÔºå‰∏ÄÊó¶ËøùËÉåÂÖ∂ÂÜÖÂú®ÈÄªËæëÔºåÁªèÊµéÂ∞±Ë¶ÅÂá∫Â§ßÈóÆÈ¢ò„ÄÇÊîπÈù©ÂºÄÊîæ‰ª•Êù•ÁöÑ‰∏âÂçÅÂ§öÂπ¥Ôºå‰∏≠ÂõΩ...\n",
      "\n",
      "[2.3] Loading student interactions...\n",
      "  ‚Üí Loading interactions from dataset/MOOCRadar/student-problem-coarse-flattened.json...\n",
      "  ‚Üí Loaded 898933 interactions from 14224 students\n",
      "‚úì Loaded 898933 interactions from 14224 students\n",
      "\n",
      "Sample student U_1002476:\n",
      "  1. Problem: Pm_668300, Concepts: ['Ëá™Áî±ÊåáÊ†á', 'Áâ©ÁêÜÂ≠¶']\n",
      "  2. Problem: Pm_668301, Concepts: ['ÊåáÊ†á', 'Áâ©ÁêÜÂ≠¶']\n",
      "  3. Problem: Pm_668304, Concepts: ['‰∏ÄÈò∂Â¢ûÈáè', 'ÂùêÊ†áÂ¢ûÈáè']\n",
      "\n",
      "======================================================================\n",
      "EXPLORATORY DATA ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset Overview (sampled):\n",
      "  ‚Ä¢ Total interactions: 94\n",
      "  ‚Ä¢ Unique users: 5\n",
      "  ‚Ä¢ Unique problems: 74\n",
      "  ‚Ä¢ Unique skills: 58\n",
      "\n",
      "üìà Correctness Distribution:\n",
      "  ‚Ä¢ Correct: 38.0 (70.4%)\n",
      "  ‚Ä¢ Incorrect: 16 (29.6%)\n",
      "\n",
      "üë§ Interactions per User:\n",
      "  ‚Ä¢ Mean: 18.8\n",
      "  ‚Ä¢ Median: 20.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Visualization saved to: assets/dataset_analysis.png\n",
      "\n",
      "======================================================================\n",
      "DATA LOADING & MAPPING COMPLETE\n",
      "======================================================================\n",
      "  ‚Üí Questions: 9384\n",
      "  ‚Üí Concepts: 9986\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: LOAD DATASET & BUILD CONCEPT MAPPINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING MOOCRADAR DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.0 Download Dataset from Google Drive (with caching)\n",
    "# -----------------------------------------------------------------------------\n",
    "def download_from_gdrive(file_id, output_path):\n",
    "    \"\"\"Download file from Google Drive if not already cached locally.\"\"\"\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\"‚úì Using cached file: {output_path}\")\n",
    "        return output_path\n",
    "    \n",
    "    print(f\"‚¨áÔ∏è  Downloading from Google Drive to {output_path}...\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        import gdown\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, output_path, quiet=False)\n",
    "        print(f\"‚úì Download complete: {output_path}\")\n",
    "        return output_path\n",
    "    except ImportError:\n",
    "        print(\"‚ö†Ô∏è  gdown not installed. Installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([os.sys.executable, \"-m\", \"pip\", \"install\", \"gdown\", \"--quiet\"])\n",
    "        import gdown\n",
    "        url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        gdown.download(url, output_path, quiet=False)\n",
    "        print(f\"‚úì Download complete: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "print(\"\\n[2.0] Checking dataset files...\")\n",
    "\n",
    "# Download/cache files\n",
    "problem_json_path = download_from_gdrive(\n",
    "    Config.GDRIVE_FILES[\"problem.json\"],\n",
    "    Config.PROBLEM_JSON\n",
    ")\n",
    "student_json_path = download_from_gdrive(\n",
    "    Config.GDRIVE_FILES[\"student-problem-coarse-flattened.json\"],\n",
    "    Config.STUDENT_JSON\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.1 Build Concept & Question Mappings using our new utilities\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[2.1] Building concept and question mappings...\")\n",
    "\n",
    "from utils import (\n",
    "    ConceptMapper, QuestionMapper, \n",
    "    build_mappings_from_dataset, load_mappings,\n",
    "    ProblemDatabase, StudentInteractions\n",
    ")\n",
    "\n",
    "# Try to load existing mappings, or build new ones\n",
    "try:\n",
    "    question_mapper, concept_mapper = load_mappings(Config.MAPPINGS_DIR)\n",
    "except FileNotFoundError:\n",
    "    print(\"  ‚Üí Building new mappings from dataset...\")\n",
    "    question_mapper, concept_mapper = build_mappings_from_dataset(\n",
    "        problem_json_path, \n",
    "        Config.MAPPINGS_DIR\n",
    "    )\n",
    "\n",
    "print(f\"\\n‚úì Question Mapper: {question_mapper.num_questions} questions\")\n",
    "print(f\"‚úì Concept Mapper: {concept_mapper.num_concepts} concepts\")\n",
    "\n",
    "# Show some sample concept mappings (Chinese ‚Üí ID)\n",
    "print(\"\\nSample Concept Mappings (Chinese ‚Üí ID):\")\n",
    "concept_stats = concept_mapper.get_statistics()\n",
    "for concept_name, count in concept_stats['top_concepts'][:5]:\n",
    "    concept_id = concept_mapper.get_id(concept_name, add_if_missing=False)\n",
    "    print(f\"  ‚Ä¢ '{concept_name}' ‚Üí CID={concept_id} (count: {count})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.2 Load Problem Database using our utility\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[2.2] Loading problem database...\")\n",
    "\n",
    "problem_db = ProblemDatabase().load_from_file(problem_json_path)\n",
    "\n",
    "# Sample problem\n",
    "sample_pid = list(problem_db.problems.keys())[0]\n",
    "sample_problem = problem_db.get_problem(sample_pid)\n",
    "print(f\"\\nSample problem structure:\")\n",
    "print(f\"  ‚Ä¢ problem_id: {sample_pid}\")\n",
    "print(f\"  ‚Ä¢ concepts: {problem_db.get_concepts(sample_pid)[:3]}...\")\n",
    "print(f\"  ‚Ä¢ text (truncated): {problem_db.get_text(sample_pid)[:100]}...\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.3 Load Student Interactions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[2.3] Loading student interactions...\")\n",
    "\n",
    "interactions = StudentInteractions().load_from_file(student_json_path)\n",
    "interactions.set_problem_database(problem_db)\n",
    "\n",
    "print(f\"‚úì Loaded {interactions.num_interactions} interactions from {interactions.num_students} students\")\n",
    "\n",
    "# Sample interaction\n",
    "sample_uid = interactions.get_all_students()[0]\n",
    "sample_seq = interactions.get_student_sequence(sample_uid, max_length=5)\n",
    "print(f\"\\nSample student {sample_uid}:\")\n",
    "for i, inter in enumerate(sample_seq[:3]):\n",
    "    pid = inter.get('problem_id', '')\n",
    "    concepts = problem_db.get_concepts(pid)\n",
    "    print(f\"  {i+1}. Problem: {pid}, Concepts: {concepts[:2]}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2.4 Exploratory Data Analysis\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "data_sample = []\n",
    "all_students = interactions.get_all_students()[:Config.MAX_USERS]\n",
    "\n",
    "for uid in all_students[:Config.MAX_USERS]:\n",
    "    for inter in interactions.get_student_sequence(uid)[:Config.MAX_INTERACTIONS // Config.MAX_USERS]:\n",
    "        pid = inter.get('problem_id', '')\n",
    "        concepts = problem_db.get_concepts(pid)\n",
    "        data_sample.append({\n",
    "            'user_id': uid,\n",
    "            'problem_id': pid,\n",
    "            'skill_id': concept_mapper.get_id(concepts[0]) if concepts else 0,\n",
    "            'is_correct': inter.get('score', inter.get('is_correct', 0))\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(data_sample)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview (sampled):\")\n",
    "print(f\"  ‚Ä¢ Total interactions: {len(df):,}\")\n",
    "print(f\"  ‚Ä¢ Unique users: {df['user_id'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Unique problems: {df['problem_id'].nunique():,}\")\n",
    "print(f\"  ‚Ä¢ Unique skills: {df['skill_id'].nunique():,}\")\n",
    "\n",
    "# Correctness distribution\n",
    "if len(df) > 0:\n",
    "    correct_rate = df['is_correct'].mean()\n",
    "    print(f\"\\nüìà Correctness Distribution:\")\n",
    "    print(f\"  ‚Ä¢ Correct: {df['is_correct'].sum():,} ({correct_rate*100:.1f}%)\")\n",
    "    print(f\"  ‚Ä¢ Incorrect: {(df['is_correct'] == 0).sum():,} ({(1-correct_rate)*100:.1f}%)\")\n",
    "\n",
    "    # Interactions per user\n",
    "    interactions_per_user = df.groupby('user_id').size()\n",
    "    print(f\"\\nüë§ Interactions per User:\")\n",
    "    print(f\"  ‚Ä¢ Mean: {interactions_per_user.mean():.1f}\")\n",
    "    print(f\"  ‚Ä¢ Median: {interactions_per_user.median():.1f}\")\n",
    "\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1. Correctness Distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    colors = ['#2ecc71', '#e74c3c']\n",
    "    correct_counts = [df['is_correct'].sum(), (df['is_correct'] == 0).sum()]\n",
    "    ax1.pie(correct_counts, labels=['Correct', 'Incorrect'], colors=colors, autopct='%1.1f%%',\n",
    "            startangle=90, explode=(0.05, 0))\n",
    "    ax1.set_title('Correctness Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # 2. Interactions per User Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(interactions_per_user, bins=min(30, len(interactions_per_user)), \n",
    "             color='#3498db', edgecolor='white', alpha=0.8)\n",
    "    ax2.set_xlabel('Interactions per User', fontsize=10)\n",
    "    ax2.set_ylabel('Count', fontsize=10)\n",
    "    ax2.set_title('User Activity Distribution', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Top Concepts by frequency\n",
    "    ax3 = axes[1, 0]\n",
    "    top_concepts = [(n, c) for n, c in concept_stats['top_concepts'][:10]]\n",
    "    if top_concepts:\n",
    "        names = [f\"C{concept_mapper.get_id(n)}\" for n, _ in top_concepts]\n",
    "        counts = [c for _, c in top_concepts]\n",
    "        ax3.barh(range(len(names)), counts, color='#9b59b6')\n",
    "        ax3.set_yticks(range(len(names)))\n",
    "        ax3.set_yticklabels(names, fontsize=8)\n",
    "        ax3.set_xlabel('Frequency', fontsize=10)\n",
    "        ax3.set_title('Top 10 Concepts', fontsize=12, fontweight='bold')\n",
    "        ax3.invert_yaxis()\n",
    "\n",
    "    # 4. Questions per concept\n",
    "    ax4 = axes[1, 1]\n",
    "    skill_counts = df['skill_id'].value_counts().head(15)\n",
    "    ax4.bar(range(len(skill_counts)), skill_counts.values, color='#e67e22')\n",
    "    ax4.set_xticks(range(len(skill_counts)))\n",
    "    ax4.set_xticklabels([f\"S{i+1}\" for i in range(len(skill_counts))], fontsize=8, rotation=45)\n",
    "    ax4.set_ylabel('Count', fontsize=10)\n",
    "    ax4.set_title('Top Skills Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('MOOCRadar Dataset Overview', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    os.makedirs('assets', exist_ok=True)\n",
    "    plt.savefig('assets/dataset_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\n‚úì Visualization saved to: assets/dataset_analysis.png\")\n",
    "\n",
    "# Store key variables for later cells\n",
    "num_questions = question_mapper.vocab_size\n",
    "num_concepts = concept_mapper.vocab_size\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA LOADING & MAPPING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  ‚Üí Questions: {num_questions}\")\n",
    "print(f\"  ‚Üí Concepts: {num_concepts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c15626",
   "metadata": {},
   "source": [
    "## Step 3: Build Dual Encoder & Train Sequence Encoder\n",
    "\n",
    "Our **Dual Encoder Architecture** consists of:\n",
    "\n",
    "1. **Context Encoder** (pre-trained, multilingual-e5-base)\n",
    "   - Encodes question text and concept descriptions\n",
    "   - Supports Chinese text from MOOCRadar dataset\n",
    "   \n",
    "2. **Sequence Encoder** (trainable Transformer)\n",
    "   - Captures student learning history patterns\n",
    "   - Generates history embeddings per student\n",
    "\n",
    "**Combined Formula:** `Embedding = Context(text) + Sequence(history)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34c14c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 3: BUILD DUAL ENCODER MODEL\n",
      "======================================================================\n",
      "\n",
      "[3.1] Importing modular components...\n",
      "‚úì Imported from models/encoders.py: ContextEncoder, SequenceEncoder, HybridEncoder\n",
      "‚úì Imported from models/embedding_model.py: QuestionEmbedding, ConceptEmbedding, KTEmbeddingModel\n",
      "‚úì Imported from utils/data_loader.py: KTDataset, create_data_loaders\n",
      "\n",
      "[3.2] Preparing data for dual encoder training...\n",
      "‚úì Prepared 5 students with sequences\n",
      "\n",
      "üìä Data validation:\n",
      "  ‚Ä¢ Max question ID in data: 5861 (vocab_size: 9384)\n",
      "  ‚Ä¢ Max concept ID in data: 6698 (vocab_size: 9986)\n",
      "\n",
      "‚úì Train: 4 | Val: 1\n",
      "\n",
      "‚úì Dual Encoder components defined:\n",
      "  ‚Ä¢ DualEncoderDataset: Handles sequence padding and batching\n",
      "  ‚Ä¢ DualEncoderDataModule: PyTorch Lightning data module\n",
      "  ‚Ä¢ SequenceEncoderLightning: Transformer-based sequence encoder\n",
      "\n",
      "Model Configuration (from Config):\n",
      "  ‚Ä¢ Embedding Dim: 768\n",
      "  ‚Ä¢ Attention Heads: 4\n",
      "  ‚Ä¢ Sequence Encoder Layers: 2\n",
      "  ‚Ä¢ Context Model: intfloat/multilingual-e5-base\n",
      "  ‚Ä¢ Combine Method: add\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: BUILD DUAL ENCODER MODEL\n",
    "# ============================================================================\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import importlib\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 3: BUILD DUAL ENCODER MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.1 Import our modules (with reload to pick up latest changes)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[3.1] Importing modular components...\")\n",
    "\n",
    "# Reload modules to pick up latest code changes\n",
    "import models\n",
    "import utils\n",
    "importlib.reload(models)\n",
    "importlib.reload(utils)\n",
    "\n",
    "from models import ContextEncoder, SequenceEncoder, HybridEncoder\n",
    "from models import QuestionEmbedding, ConceptEmbedding, KTEmbeddingModel\n",
    "from utils import KTDataset, create_data_loaders\n",
    "\n",
    "print(\"‚úì Imported from models/encoders.py: ContextEncoder, SequenceEncoder, HybridEncoder\")\n",
    "print(\"‚úì Imported from models/embedding_model.py: QuestionEmbedding, ConceptEmbedding, KTEmbeddingModel\")\n",
    "print(\"‚úì Imported from utils/data_loader.py: KTDataset, create_data_loaders\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.2 Prepare Data for Training\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[3.2] Preparing data for dual encoder training...\")\n",
    "\n",
    "# Build students_data from our loaded interactions\n",
    "students_data = []\n",
    "all_students = interactions.get_all_students()[:Config.MAX_USERS]\n",
    "\n",
    "for uid in all_students:\n",
    "    student_seq = interactions.get_student_sequence(uid, max_length=Config.MAX_SEQ_LEN)\n",
    "    if len(student_seq) < 5:  # Skip students with too few interactions\n",
    "        continue\n",
    "    \n",
    "    questions = []\n",
    "    concepts = []\n",
    "    correctness = []\n",
    "    question_texts = []\n",
    "    concept_texts = []\n",
    "    \n",
    "    for inter in student_seq:\n",
    "        pid = inter.get('problem_id', '')\n",
    "        concept_list = problem_db.get_concepts(pid)\n",
    "        \n",
    "        # Get numeric IDs - DON'T add new ones, use 0 for unknown\n",
    "        qid = question_mapper.get_id(pid, add_if_missing=False)\n",
    "        if qid == -1:  # Unknown question\n",
    "            qid = 0\n",
    "            \n",
    "        if concept_list:\n",
    "            cid = concept_mapper.get_id(concept_list[0], add_if_missing=False)\n",
    "            if cid == -1:  # Unknown concept\n",
    "                cid = 0\n",
    "        else:\n",
    "            cid = 0\n",
    "        \n",
    "        # Handle correctness with None value check\n",
    "        score = inter.get('score')\n",
    "        if score is None:\n",
    "            score = inter.get('is_correct', 0)\n",
    "        is_correct = int(score) if score is not None else 0\n",
    "        \n",
    "        questions.append(qid)\n",
    "        concepts.append(cid)\n",
    "        correctness.append(is_correct)\n",
    "        \n",
    "        # Store text for context encoding\n",
    "        question_texts.append(problem_db.get_text(pid)[:512])  # Truncate long text\n",
    "        concept_texts.append(concept_list[0] if concept_list else \"Unknown\")\n",
    "    \n",
    "    students_data.append({\n",
    "        'user_id': uid,\n",
    "        'questions': questions,\n",
    "        'concepts': concepts,\n",
    "        'correctness': correctness,\n",
    "        'question_texts': question_texts,\n",
    "        'concept_texts': concept_texts\n",
    "    })\n",
    "\n",
    "print(f\"‚úì Prepared {len(students_data)} students with sequences\")\n",
    "\n",
    "# Validate IDs are within range\n",
    "max_qid = max(max(s['questions']) for s in students_data)\n",
    "max_cid = max(max(s['concepts']) for s in students_data)\n",
    "print(f\"\\nüìä Data validation:\")\n",
    "print(f\"  ‚Ä¢ Max question ID in data: {max_qid} (vocab_size: {num_questions})\")\n",
    "print(f\"  ‚Ä¢ Max concept ID in data: {max_cid} (vocab_size: {num_concepts})\")\n",
    "\n",
    "if max_qid >= num_questions:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNING: Max question ID ({max_qid}) >= vocab_size ({num_questions})\")\n",
    "if max_cid >= num_concepts:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNING: Max concept ID ({max_cid}) >= vocab_size ({num_concepts})\")\n",
    "\n",
    "# Split into train/val\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, val_data = train_test_split(\n",
    "    students_data, \n",
    "    test_size=Config.TEST_SIZE, \n",
    "    random_state=Config.RANDOM_SEED\n",
    ")\n",
    "print(f\"\\n‚úì Train: {len(train_data)} | Val: {len(val_data)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.3 Dataset Class for Dual Encoder Training\n",
    "# -----------------------------------------------------------------------------\n",
    "class DualEncoderDataset(Dataset):\n",
    "    \"\"\"Dataset for Dual Encoder (Context + Sequence) training\"\"\"\n",
    "    def __init__(self, students_data, max_seq_len=100):\n",
    "        self.students = students_data\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.students)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        student = self.students[idx]\n",
    "        questions = student['questions'][-self.max_seq_len:]\n",
    "        concepts = student['concepts'][-self.max_seq_len:]\n",
    "        correctness = student['correctness'][-self.max_seq_len:]\n",
    "        \n",
    "        seq_len = len(questions)\n",
    "        pad_len = self.max_seq_len - seq_len\n",
    "        \n",
    "        # Pad sequences\n",
    "        questions = questions + [0] * pad_len\n",
    "        concepts = concepts + [0] * pad_len\n",
    "        correctness = correctness + [0] * pad_len\n",
    "        mask = [1] * seq_len + [0] * pad_len\n",
    "        \n",
    "        return {\n",
    "            'questions': torch.LongTensor(questions),\n",
    "            'concepts': torch.LongTensor(concepts),\n",
    "            'correctness': torch.FloatTensor(correctness),\n",
    "            'mask': torch.FloatTensor(mask),\n",
    "            'user_id': student['user_id']\n",
    "        }\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.4 Sequence Encoder Lightning Module\n",
    "# -----------------------------------------------------------------------------\n",
    "class SequenceEncoderLightning(pl.LightningModule):\n",
    "    \"\"\"Sequence Encoder for learning history - PyTorch Lightning wrapper\"\"\"\n",
    "    \n",
    "    def __init__(self, num_questions, num_concepts, embed_dim=768, num_heads=8, \n",
    "                 num_layers=2, dropout=0.15, learning_rate=5e-5, weight_decay=1e-4,\n",
    "                 warmup_epochs=5, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # Use our SequenceEncoder from models/\n",
    "        self.encoder = SequenceEncoder(\n",
    "            num_questions=num_questions,\n",
    "            num_concepts=num_concepts,\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            max_seq_len=Config.MAX_SEQ_LEN\n",
    "        )\n",
    "        \n",
    "        # Output layer for prediction\n",
    "        self.output_layer = nn.Linear(embed_dim, 1)\n",
    "        \n",
    "    def forward(self, questions, concepts, correctness, mask):\n",
    "        # Get sequence embeddings (returns tuple: question_emb, concept_emb)\n",
    "        # We use question embeddings for knowledge tracing task\n",
    "        question_hidden, concept_hidden = self.encoder(\n",
    "            questions, concepts, correctness, mask, return_all_positions=True\n",
    "        )\n",
    "        \n",
    "        # Use question hidden states for prediction\n",
    "        output = self.output_layer(question_hidden).squeeze(-1)\n",
    "        \n",
    "        return output, question_hidden\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        questions = batch['questions']\n",
    "        concepts = batch['concepts']\n",
    "        correctness = batch['correctness']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        predictions, _ = self(questions, concepts, correctness, mask)\n",
    "        \n",
    "        # Shift for next-step prediction\n",
    "        pred_shifted = predictions[:, :-1]\n",
    "        target_shifted = correctness[:, 1:]\n",
    "        mask_shifted = mask[:, 1:]\n",
    "        \n",
    "        loss = self._compute_loss(pred_shifted, target_shifted, mask_shifted)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_binary = (torch.sigmoid(pred_shifted) > 0.5).float()\n",
    "            correct = ((pred_binary == target_shifted) * mask_shifted).sum()\n",
    "            total = mask_shifted.sum()\n",
    "            acc = correct / (total + 1e-8)\n",
    "        \n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        questions = batch['questions']\n",
    "        concepts = batch['concepts']\n",
    "        correctness = batch['correctness']\n",
    "        mask = batch['mask']\n",
    "        \n",
    "        predictions, _ = self(questions, concepts, correctness, mask)\n",
    "        \n",
    "        pred_shifted = predictions[:, :-1]\n",
    "        target_shifted = correctness[:, 1:]\n",
    "        mask_shifted = mask[:, 1:]\n",
    "        \n",
    "        loss = self._compute_loss(pred_shifted, target_shifted, mask_shifted)\n",
    "        \n",
    "        pred_binary = (torch.sigmoid(pred_shifted) > 0.5).float()\n",
    "        correct = ((pred_binary == target_shifted) * mask_shifted).sum()\n",
    "        total = mask_shifted.sum()\n",
    "        acc = correct / (total + 1e-8)\n",
    "        \n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_loss(self, predictions, targets, mask):\n",
    "        smooth_targets = targets * (1 - self.label_smoothing) + 0.5 * self.label_smoothing\n",
    "        loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "            predictions, smooth_targets, reduction='none'\n",
    "        )\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "        return masked_loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=self.warmup_epochs, T_mult=2\n",
    "        )\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    def get_embeddings(self, dataloader):\n",
    "        \"\"\"Extract sequence embeddings for all students\"\"\"\n",
    "        self.eval()\n",
    "        embeddings = []\n",
    "        user_ids = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                questions = batch['questions'].to(self.device)\n",
    "                concepts = batch['concepts'].to(self.device)\n",
    "                correctness = batch['correctness'].to(self.device)\n",
    "                mask = batch['mask'].to(self.device)\n",
    "                \n",
    "                _, hidden = self(questions, concepts, correctness, mask)\n",
    "                \n",
    "                # Get last valid position embedding for each student\n",
    "                for i in range(hidden.size(0)):\n",
    "                    valid_len = int(mask[i].sum().item())\n",
    "                    if valid_len > 0:\n",
    "                        student_emb = hidden[i, valid_len - 1, :].cpu().numpy()\n",
    "                    else:\n",
    "                        student_emb = hidden[i, 0, :].cpu().numpy()\n",
    "                    embeddings.append(student_emb)\n",
    "                    user_ids.append(batch['user_id'][i])\n",
    "        \n",
    "        return np.array(embeddings), user_ids\n",
    "\n",
    "# Create data module\n",
    "class DualEncoderDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, batch_size=64, max_seq_len=100):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = DualEncoderDataset(self.train_data, self.max_seq_len)\n",
    "        self.val_dataset = DualEncoderDataset(self.val_data, self.max_seq_len)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"\\n‚úì Dual Encoder components defined:\")\n",
    "print(f\"  ‚Ä¢ DualEncoderDataset: Handles sequence padding and batching\")\n",
    "print(f\"  ‚Ä¢ DualEncoderDataModule: PyTorch Lightning data module\")\n",
    "print(f\"  ‚Ä¢ SequenceEncoderLightning: Transformer-based sequence encoder\")\n",
    "print(f\"\\nModel Configuration (from Config):\")\n",
    "print(f\"  ‚Ä¢ Embedding Dim: {Config.EMBED_DIM}\")\n",
    "print(f\"  ‚Ä¢ Attention Heads: {Config.NUM_HEADS}\")\n",
    "print(f\"  ‚Ä¢ Sequence Encoder Layers: {Config.NUM_SEQ_LAYERS}\")\n",
    "print(f\"  ‚Ä¢ Context Model: {Config.CONTEXT_MODEL}\")\n",
    "print(f\"  ‚Ä¢ Combine Method: {Config.COMBINE_METHOD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1acb33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TRAINING SEQUENCE ENCODER\n",
      "======================================================================\n",
      "\n",
      "[3.4.1] Setting up data module...\n",
      "‚úì Train: 4 students\n",
      "‚úì Val: 1 students\n",
      "\n",
      "[3.4.2] Initializing Sequence Encoder model...\n",
      "  ‚Üí SequenceEncoder initialized:\n",
      "     Questions: 9384, Concepts: 9986\n",
      "     Embed dim: 768, Layers: 2, Heads: 4\n",
      "‚úì Model initialized with 30,277,633 trainable parameters\n",
      "\n",
      "[3.4.3] Setting up training callbacks...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">SeqEncoder-small</strong> at: <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/l31thgkd' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/l31thgkd</a><br> View project at: <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260205_172930-l31thgkd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\wandb\\run-20260205_173623-ju6pajd1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/ju6pajd1' target=\"_blank\">SeqEncoder-small</a></strong> to <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/ju6pajd1' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/ju6pajd1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "üí° Tip: For seamless cloud logging and experiment tracking, try installing [litlogger](https://pypi.org/project/litlogger/) to enable LitLogger, which logs metrics and artifacts automatically to the Lightning Experiments platform.\n",
      "\n",
      "  | Name         | Type            | Params | Mode  | FLOPs\n",
      "-----------------------------------------------------------------\n",
      "0 | encoder      | SequenceEncoder | 30.3 M | train | 0    \n",
      "1 | output_layer | Linear          | 769    | train | 0    \n",
      "-----------------------------------------------------------------\n",
      "30.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "30.3 M    Total params\n",
      "121.111   Total estimated model params size (MB)\n",
      "32        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Wandb initialized: SeqEncoder-small\n",
      "  ‚Üí View at: https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/ju6pajd1\n",
      "\n",
      "[3.4.4] Starting training...\n",
      "\n",
      "üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è üèãÔ∏è \n",
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s]                             "
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m     91\u001b[39m trainer = pl.Trainer(\n\u001b[32m     92\u001b[39m     max_epochs=Config.AKT_EPOCHS,\n\u001b[32m     93\u001b[39m     callbacks=[checkpoint_callback, early_stop_callback],\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     gradient_clip_algorithm=\u001b[33m'\u001b[39m\u001b[33mnorm\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    103\u001b[39m )\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33müèãÔ∏è \u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m35\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_encoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müèãÔ∏è \u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m35\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úì Training completed!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:584\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    582\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    593\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:49\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     48\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     52\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:630\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path, weights_only)\u001b[39m\n\u001b[32m    623\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    624\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    625\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    626\u001b[39m     ckpt_path,\n\u001b[32m    627\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    628\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    629\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    633\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1079\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path, weights_only)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1076\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1077\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1078\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1084\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1123\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1121\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1125\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:217\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    216\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:465\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_epoch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    464\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m465\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:153\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m         \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:352\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    349\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33m\"\u001b[39m\u001b[33mrun_training_batch\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    351\u001b[39m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m         batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    354\u001b[39m         batch_output = \u001b[38;5;28mself\u001b[39m.manual_optimization.run(kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    185\u001b[39m         closure()\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_ready()\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[32m    280\u001b[39m     \u001b[38;5;28mself\u001b[39m.optim_progress.optimizer.step.increment_completed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:177\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m pl_module._current_fx_name = hook_name\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    180\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1368\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1337\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1339\u001b[39m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1342\u001b[39m     optimizer_closure: Optional[Callable[[], Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1343\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1344\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1345\u001b[39m \u001b[33;03m    the optimizer.\u001b[39;00m\n\u001b[32m   1346\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1366\u001b[39m \n\u001b[32m   1367\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\amp.py:76\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimizer_step\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     72\u001b[39m     **kwargs: Any,\n\u001b[32m     73\u001b[39m ) -> Any:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;66;03m# skip scaler logic, as bfloat16 does not require scaler\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(optimizer, LBFGS):\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:166\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m opt = opt_ref()\n\u001b[32m    165\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:526\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    521\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    522\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m             )\n\u001b[32m    525\u001b[39m \u001b[38;5;66;03m# pyrefly: ignore [invalid-param-spec]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:81\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     80\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     83\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:227\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    226\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n\u001b[32m    230\u001b[39m     params_with_grad: \u001b[38;5;28mlist\u001b[39m[Tensor] = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrap_closure\u001b[39m(\n\u001b[32m     97\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     98\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m     optimizer: Steppable,\n\u001b[32m    100\u001b[39m     closure: Callable[[], Any],\n\u001b[32m    101\u001b[39m ) -> Any:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    hook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m     \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    134\u001b[39m         \u001b[38;5;28mself\u001b[39m.warning_cache.warn(\u001b[33m\"\u001b[39m\u001b[33m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Performs the actual train step with the tied hooks.\u001b[39;00m\n\u001b[32m    309\u001b[39m \n\u001b[32m    310\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    315\u001b[39m \n\u001b[32m    316\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training_step_output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m trainer.world_size > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:329\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m    332\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model != \u001b[38;5;28mself\u001b[39m.lightning_module:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 199\u001b[39m, in \u001b[36mSequenceEncoderLightning.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    196\u001b[39m correctness = batch[\u001b[33m'\u001b[39m\u001b[33mcorrectness\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    197\u001b[39m mask = batch[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m predictions, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[38;5;66;03m# Shift for next-step prediction\u001b[39;00m\n\u001b[32m    202\u001b[39m pred_shifted = predictions[:, :-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 184\u001b[39m, in \u001b[36mSequenceEncoderLightning.forward\u001b[39m\u001b[34m(self, questions, concepts, correctness, mask)\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, questions, concepts, correctness, mask):\n\u001b[32m    182\u001b[39m     \u001b[38;5;66;03m# Get sequence embeddings (returns tuple: question_emb, concept_emb)\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;66;03m# We use question embeddings for knowledge tracing task\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     question_hidden, concept_hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorrectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_positions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;66;03m# Use question hidden states for prediction\u001b[39;00m\n\u001b[32m    189\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.output_layer(question_hidden).squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\models\\encoders.py:291\u001b[39m, in \u001b[36mSequenceEncoder.forward\u001b[39m\u001b[34m(self, question_ids, concept_ids, responses, mask, return_all_positions)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_response \u001b[38;5;129;01mand\u001b[39;00m responses \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    290\u001b[39m     response_idx = (responses + \u001b[32m1\u001b[39m).long()  \u001b[38;5;66;03m# 0->1 (incorrect), 1->2 (correct)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     r_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresponse_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     combined = combined + r_emb\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Create attention masks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:191\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:2567\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2561\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2563\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2564\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2565\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2566\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2567\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3.4: TRAIN SEQUENCE ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING SEQUENCE ENCODER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Initialize data module\n",
    "print(\"\\n[3.4.1] Setting up data module...\")\n",
    "data_module = DualEncoderDataModule(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    batch_size=Config.AKT_BATCH_SIZE, \n",
    "    max_seq_len=Config.MAX_SEQ_LEN\n",
    ")\n",
    "data_module.setup()\n",
    "print(f\"‚úì Train: {len(data_module.train_dataset)} students\")\n",
    "print(f\"‚úì Val: {len(data_module.val_dataset)} students\")\n",
    "\n",
    "# Initialize sequence encoder model\n",
    "print(\"\\n[3.4.2] Initializing Sequence Encoder model...\")\n",
    "seq_encoder_model = SequenceEncoderLightning(\n",
    "    num_questions=num_questions,\n",
    "    num_concepts=num_concepts,\n",
    "    embed_dim=Config.EMBED_DIM,\n",
    "    num_heads=Config.NUM_HEADS,\n",
    "    num_layers=Config.NUM_SEQ_LAYERS,\n",
    "    dropout=Config.AKT_DROPOUT,\n",
    "    learning_rate=Config.AKT_LR,\n",
    "    weight_decay=Config.AKT_WEIGHT_DECAY,\n",
    "    warmup_epochs=Config.AKT_WARMUP_EPOCHS,\n",
    "    label_smoothing=Config.AKT_LABEL_SMOOTHING\n",
    ")\n",
    "\n",
    "trainable_params = sum(p.numel() for p in seq_encoder_model.parameters() if p.requires_grad)\n",
    "print(f\"‚úì Model initialized with {trainable_params:,} trainable parameters\")\n",
    "\n",
    "# Setup callbacks\n",
    "print(\"\\n[3.4.3] Setting up training callbacks...\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath=Config.AKT_CHECKPOINT_DIR,\n",
    "    filename='seq-encoder-{epoch:02d}-{val_acc:.4f}',\n",
    "    save_top_k=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_acc',\n",
    "    patience=Config.AKT_EARLY_STOP_PATIENCE,\n",
    "    mode='max',\n",
    "    min_delta=0.001\n",
    ")\n",
    "\n",
    "# Setup loggers\n",
    "csv_logger = CSVLogger(Config.AKT_LOG_DIR, name='seq_encoder_training')\n",
    "loggers = [csv_logger]\n",
    "\n",
    "# Initialize wandb if enabled\n",
    "if Config.USE_WANDB:\n",
    "    wandb_run = wandb.init(\n",
    "        entity=Config.WANDB_ENTITY,\n",
    "        project=Config.WANDB_PROJECT,\n",
    "        name=f\"SeqEncoder-{Config.ACTIVE_PRESET}\",\n",
    "        config={\n",
    "            \"preset\": Config.ACTIVE_PRESET,\n",
    "            \"model\": \"SequenceEncoder\",\n",
    "            \"embed_dim\": Config.EMBED_DIM,\n",
    "            \"num_heads\": Config.NUM_HEADS,\n",
    "            \"num_layers\": Config.NUM_SEQ_LAYERS,\n",
    "            \"batch_size\": Config.AKT_BATCH_SIZE,\n",
    "            \"learning_rate\": Config.AKT_LR,\n",
    "            \"epochs\": Config.AKT_EPOCHS,\n",
    "            \"num_students\": len(students_data),\n",
    "            \"num_questions\": num_questions,\n",
    "            \"num_concepts\": num_concepts,\n",
    "            \"context_model\": Config.CONTEXT_MODEL,\n",
    "        }\n",
    "    )\n",
    "    wandb_logger = WandbLogger(experiment=wandb_run)\n",
    "    loggers.append(wandb_logger)\n",
    "    print(f\"‚úì Wandb initialized: {wandb_run.name}\")\n",
    "    print(f\"  ‚Üí View at: {wandb_run.get_url()}\")\n",
    "\n",
    "# Initialize trainer\n",
    "print(\"\\n[3.4.4] Starting training...\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=Config.AKT_EPOCHS,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    logger=loggers,\n",
    "    accelerator='auto' if Config.USE_GPU else 'cpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True,\n",
    "    accumulate_grad_batches=Config.GRADIENT_ACCUMULATION,\n",
    "    precision='16-mixed' if Config.MIXED_PRECISION else 32,\n",
    "    gradient_clip_val=1.0,\n",
    "    gradient_clip_algorithm='norm'\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"üèãÔ∏è \" * 35)\n",
    "trainer.fit(seq_encoder_model, data_module)\n",
    "print(\"üèãÔ∏è \" * 35)\n",
    "\n",
    "print(f\"\\n‚úì Training completed!\")\n",
    "print(f\"‚úì Best checkpoint: {checkpoint_callback.best_model_path}\")\n",
    "print(f\"‚úì Best validation accuracy: {checkpoint_callback.best_model_score:.4f}\")\n",
    "\n",
    "# Load best model\n",
    "print(\"\\n[3.4.5] Loading best model for embedding extraction...\")\n",
    "best_seq_model = SequenceEncoderLightning.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path,\n",
    "    num_questions=num_questions,\n",
    "    num_concepts=num_concepts\n",
    ")\n",
    "print(\"‚úì Best sequence encoder model loaded\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3.5 Plot Training Curves\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[3.5] Plotting training curves...\")\n",
    "\n",
    "metrics_path = f'{Config.AKT_LOG_DIR}/seq_encoder_training/version_{csv_logger.version}/metrics.csv'\n",
    "if os.path.exists(metrics_path):\n",
    "    metrics_df = pd.read_csv(metrics_path)\n",
    "    print(f\"Available columns: {list(metrics_df.columns)}\")\n",
    "    \n",
    "    train_loss_col = None\n",
    "    train_acc_col = None\n",
    "    val_loss_col = None\n",
    "    val_acc_col = None\n",
    "    \n",
    "    for col in metrics_df.columns:\n",
    "        if 'train' in col.lower() and 'loss' in col.lower():\n",
    "            train_loss_col = col\n",
    "        elif 'train' in col.lower() and 'acc' in col.lower():\n",
    "            train_acc_col = col\n",
    "        elif 'val' in col.lower() and 'loss' in col.lower():\n",
    "            val_loss_col = col\n",
    "        elif 'val' in col.lower() and 'acc' in col.lower():\n",
    "            val_acc_col = col\n",
    "    \n",
    "    if train_loss_col and val_loss_col:\n",
    "        train_metrics = metrics_df[metrics_df[train_loss_col].notna()][['epoch', train_loss_col, train_acc_col]]\n",
    "        train_metrics.columns = ['epoch', 'train_loss', 'train_acc']\n",
    "        \n",
    "        val_metrics = metrics_df[metrics_df[val_loss_col].notna()][['epoch', val_loss_col, val_acc_col]]\n",
    "        val_metrics.columns = ['epoch', 'val_loss', 'val_acc']\n",
    "        \n",
    "        metrics_combined = pd.merge(\n",
    "            train_metrics.groupby('epoch').mean().reset_index(),\n",
    "            val_metrics.groupby('epoch').mean().reset_index(),\n",
    "            on='epoch'\n",
    "        )\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        axes[0].plot(metrics_combined['epoch'], metrics_combined['train_loss'], 'b-o', label='Train', markersize=6)\n",
    "        axes[0].plot(metrics_combined['epoch'], metrics_combined['val_loss'], 'r-s', label='Val', markersize=6)\n",
    "        axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[0].set_ylabel('Loss', fontsize=11)\n",
    "        axes[0].set_title('Sequence Encoder Training - Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[1].plot(metrics_combined['epoch'], metrics_combined['train_acc'], 'b-o', label='Train', markersize=6)\n",
    "        axes[1].plot(metrics_combined['epoch'], metrics_combined['val_acc'], 'r-s', label='Val', markersize=6)\n",
    "        axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "        axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "        axes[1].set_title('Sequence Encoder Training - Accuracy', fontsize=12, fontweight='bold')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('assets/seq_encoder_training.png', dpi=300, bbox_inches='tight')\n",
    "        print(\"‚úì Training curves saved as 'assets/seq_encoder_training.png'\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Could not find expected metric columns in CSV.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Metrics file not found. Skipping plot.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì SEQUENCE ENCODER TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a567822",
   "metadata": {},
   "source": [
    "## Step 4: Build Context Encoder & Extract Combined Embeddings\n",
    "\n",
    "Now we build the **Context Encoder** and combine with Sequence embeddings:\n",
    "\n",
    "1. **Context Embeddings** (from multilingual-e5-base)\n",
    "   - Question text embedding via pre-trained model\n",
    "   - Concept description embedding\n",
    "   - Dimension: `EMBED_DIM` (768)\n",
    "\n",
    "2. **Sequence Embeddings** (from trained Sequence Encoder)\n",
    "   - Student learning history patterns\n",
    "   - Dimension: `EMBED_DIM`\n",
    "\n",
    "**Combined:** `Question_i = Context(text) + Sequence(history)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6fba96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 4: EXTRACT EMBEDDINGS FOR LLM FINE-TUNING\n",
      "======================================================================\n",
      "\n",
      "[4.1] Extracting student history embeddings from AKT...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Train embeddings shape: (2, 768)\n",
      "‚úì Val embeddings shape: (1, 768)\n",
      "‚úì Embedding dimension: 768\n",
      "\n",
      "‚úì Embeddings saved to:\n",
      "  ‚Ä¢ embeddings/small/student_embeddings_train.npy\n",
      "  ‚Ä¢ embeddings/small/student_embeddings_val.npy\n",
      "\n",
      "[4.2] Designing embedding adapter...\n",
      "\n",
      "Embedding Adapter Configuration:\n",
      "  ‚Ä¢ AKT Embedding Dim: 768\n",
      "  ‚Ä¢ LLM Hidden Size: 2048\n",
      "  ‚Ä¢ Adapter Hidden Dim: 1408\n",
      "  ‚Ä¢ Num Soft Tokens: 4\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
      "‚îÇ                    EMBEDDING FLOW ARCHITECTURE                       ‚îÇ\n",
      "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
      "‚îÇ                                                                       ‚îÇ\n",
      "‚îÇ   STUDENT HISTORY CONTEXT (from AKT):                                ‚îÇ\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
      "‚îÇ   ‚îÇ  AKT Embeddings ‚îÇ (768-dim)                             ‚îÇ\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ            ‚ñº                                                          ‚îÇ\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
      "‚îÇ   ‚îÇ History Adapter ‚îÇ MLP: 768 ‚Üí 2048           ‚îÇ\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ            ‚ñº                                                          ‚îÇ\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
      "‚îÇ   ‚îÇ  Soft Tokens    ‚îÇ (4 tokens √ó 2048-dim)        ‚îÇ\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ   QUESTION CONTEXT (from LLM tokenizer):                             ‚îÇ\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
      "‚îÇ   ‚îÇ  Text Tokens    ‚îÇ (question content, concepts, etc.)             ‚îÇ\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                ‚îÇ\n",
      "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
      "‚îÇ            ‚ñº                                                          ‚îÇ\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ\n",
      "‚îÇ   ‚îÇ  [SOFT_TOKENS] + [TEXT_TOKENS]          ‚îÇ                        ‚îÇ\n",
      "‚îÇ   ‚îÇ              ‚Üì                          ‚îÇ                        ‚îÇ\n",
      "‚îÇ   ‚îÇ     LLM (TinyLlama-1.1B-Chat-v1.0)                           ‚îÇ\n",
      "‚îÇ   ‚îÇ         (with LoRA)                     ‚îÇ                        ‚îÇ\n",
      "‚îÇ   ‚îÇ              ‚Üì                          ‚îÇ                        ‚îÇ\n",
      "‚îÇ   ‚îÇ        Classifier                       ‚îÇ                        ‚îÇ\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ\n",
      "‚îÇ                       ‚Üì                                              ‚îÇ\n",
      "‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ\n",
      "‚îÇ              ‚îÇ Correct (1)   ‚îÇ                                       ‚îÇ\n",
      "‚îÇ              ‚îÇ Incorrect (0) ‚îÇ                                       ‚îÇ\n",
      "‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ\n",
      "‚îÇ                                                                       ‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\n",
      "======================================================================\n",
      "‚úì EMBEDDING EXTRACTION COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: BUILD CONTEXT ENCODER & EXTRACT COMBINED EMBEDDINGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 4: BUILD CONTEXT ENCODER & EXTRACT EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.1 Initialize Context Encoder\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[4.1] Initializing Context Encoder...\")\n",
    "\n",
    "context_encoder = ContextEncoder(\n",
    "    model_name=Config.CONTEXT_MODEL,\n",
    "    embed_dim=Config.EMBED_DIM,\n",
    "    freeze=Config.FREEZE_CONTEXT\n",
    ")\n",
    "\n",
    "print(f\"‚úì Context Encoder initialized:\")\n",
    "print(f\"  ‚Ä¢ Model: {Config.CONTEXT_MODEL}\")\n",
    "print(f\"  ‚Ä¢ Output dim: {Config.EMBED_DIM}\")\n",
    "print(f\"  ‚Ä¢ Frozen: {Config.FREEZE_CONTEXT}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.2 Extract Sequence Embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[4.2] Extracting sequence embeddings...\")\n",
    "\n",
    "best_seq_model.eval()\n",
    "train_seq_embeddings, train_user_ids = best_seq_model.get_embeddings(data_module.train_dataloader())\n",
    "val_seq_embeddings, val_user_ids = best_seq_model.get_embeddings(data_module.val_dataloader())\n",
    "\n",
    "print(f\"‚úì Train sequence embeddings: {train_seq_embeddings.shape}\")\n",
    "print(f\"‚úì Val sequence embeddings: {val_seq_embeddings.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.3 Extract Context Embeddings for unique questions/concepts\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[4.3] Extracting context embeddings for questions and concepts...\")\n",
    "\n",
    "# Get unique question texts\n",
    "unique_questions = {}\n",
    "unique_concepts = {}\n",
    "\n",
    "for student in students_data:\n",
    "    for qid, cid, q_text, c_text in zip(\n",
    "        student['questions'], \n",
    "        student['concepts'],\n",
    "        student['question_texts'],\n",
    "        student['concept_texts']\n",
    "    ):\n",
    "        if qid not in unique_questions:\n",
    "            unique_questions[qid] = q_text\n",
    "        if cid not in unique_concepts:\n",
    "            unique_concepts[cid] = c_text\n",
    "\n",
    "print(f\"  ‚Ä¢ Unique questions: {len(unique_questions)}\")\n",
    "print(f\"  ‚Ä¢ Unique concepts: {len(unique_concepts)}\")\n",
    "\n",
    "# Extract context embeddings\n",
    "print(\"  ‚Üí Encoding question texts...\")\n",
    "question_context_embeddings = {}\n",
    "context_encoder.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for qid, text in unique_questions.items():\n",
    "        emb = context_encoder([text]).cpu().numpy()\n",
    "        question_context_embeddings[qid] = emb[0]\n",
    "\n",
    "print(\"  ‚Üí Encoding concept texts...\")\n",
    "concept_context_embeddings = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for cid, text in unique_concepts.items():\n",
    "        emb = context_encoder([text]).cpu().numpy()\n",
    "        concept_context_embeddings[cid] = emb[0]\n",
    "\n",
    "print(f\"‚úì Question context embeddings: {len(question_context_embeddings)}\")\n",
    "print(f\"‚úì Concept context embeddings: {len(concept_context_embeddings)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.4 Save Embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[4.4] Saving embeddings...\")\n",
    "\n",
    "os.makedirs(os.path.dirname(Config.TRAIN_EMBEDDINGS), exist_ok=True)\n",
    "os.makedirs('cache/embeddings', exist_ok=True)\n",
    "\n",
    "# Save sequence embeddings\n",
    "np.save(Config.TRAIN_EMBEDDINGS, train_seq_embeddings)\n",
    "np.save(Config.VAL_EMBEDDINGS, val_seq_embeddings)\n",
    "\n",
    "# Save context embeddings\n",
    "np.save('cache/embeddings/question_context.npy', \n",
    "        {k: v for k, v in question_context_embeddings.items()})\n",
    "np.save('cache/embeddings/concept_context.npy',\n",
    "        {k: v for k, v in concept_context_embeddings.items()})\n",
    "\n",
    "print(f\"\\n‚úì Embeddings saved:\")\n",
    "print(f\"  ‚Ä¢ Sequence: {Config.TRAIN_EMBEDDINGS}, {Config.VAL_EMBEDDINGS}\")\n",
    "print(f\"  ‚Ä¢ Context: cache/embeddings/question_context.npy, concept_context.npy\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4.5 Display Architecture Diagram\n",
    "# -----------------------------------------------------------------------------\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ               DUAL ENCODER EMBEDDING ARCHITECTURE                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                                                       ‚îÇ\n",
    "‚îÇ   CONTEXT ENCODER ({Config.CONTEXT_MODEL.split('/')[-1]}):                              ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ   ‚îÇ Question Text   ‚îÇ ‚Üí Encoder ‚Üí Context Emb ({Config.EMBED_DIM}-dim)     ‚îÇ\n",
    "‚îÇ   ‚îÇ Concept Text    ‚îÇ ‚Üí Encoder ‚Üí Context Emb ({Config.EMBED_DIM}-dim)     ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ   SEQUENCE ENCODER (Trained Transformer):                           ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ\n",
    "‚îÇ   ‚îÇ Learning History‚îÇ ‚Üí Encoder ‚Üí Sequence Emb ({Config.EMBED_DIM}-dim)    ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ   COMBINATION ({Config.COMBINE_METHOD}):                                            ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ\n",
    "‚îÇ   ‚îÇ Question Emb = Context + Sequence       ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îÇ Concept Emb = Context + Sequence        ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ\n",
    "‚îÇ            ‚îÇ                                                          ‚îÇ\n",
    "‚îÇ            ‚ñº                                                          ‚îÇ\n",
    "‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ\n",
    "‚îÇ   ‚îÇ      [QuesEmbed] + [ConcEmbed]          ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îÇ              ‚Üì                          ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îÇ     LLM (with LoRA fine-tuning)         ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îÇ              ‚Üì                          ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îÇ      Predict: Yes / No                  ‚îÇ                        ‚îÇ\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ\n",
    "‚îÇ                                                                       ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úì EMBEDDING EXTRACTION COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec953033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5: DEFINE LLM MODEL WITH EMBEDDING ADAPTER\n",
      "======================================================================\n",
      "\n",
      "‚úì KnowledgeTracingLLM class defined\n",
      "‚úì KTLlamaDataset class defined\n",
      "\n",
      "Using LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "Embedding: 768 ‚Üí 2048\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: LLM MODEL DEFINITION (USING MODULAR COMPONENTS)\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5: DEFINE LLM MODEL WITH DUAL ENCODER EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.1 Import our modular KnowledgeTracingLLM\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.1] Importing modular LLM components...\")\n",
    "\n",
    "from models import KnowledgeTracingLLM, PromptFormatter, KTEmbeddingModel\n",
    "\n",
    "print(\"‚úì Imported from models/kt_llm.py: KnowledgeTracingLLM, PromptFormatter\")\n",
    "print(\"‚úì Imported from models/embedding_model.py: KTEmbeddingModel\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2 Build KT Embedding Model (combines Context + Sequence encoders)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2] Building KT Embedding Model...\")\n",
    "\n",
    "# Initialize the embedding model\n",
    "kt_embedding_model = KTEmbeddingModel(\n",
    "    num_questions=num_questions,\n",
    "    num_concepts=num_concepts,\n",
    "    embed_dim=Config.EMBED_DIM,\n",
    "    context_model=Config.CONTEXT_MODEL,\n",
    "    num_seq_layers=Config.NUM_SEQ_LAYERS,\n",
    "    num_heads=Config.NUM_HEADS,\n",
    "    dropout=Config.AKT_DROPOUT,\n",
    "    combine_method=Config.COMBINE_METHOD,\n",
    "    freeze_context=Config.FREEZE_CONTEXT\n",
    ")\n",
    "\n",
    "# Load the trained sequence encoder weights\n",
    "print(\"  ‚Üí Loading trained sequence encoder weights...\")\n",
    "seq_encoder_state = best_seq_model.encoder.state_dict()\n",
    "kt_embedding_model.question_embedding.sequence_encoder.load_state_dict(seq_encoder_state)\n",
    "kt_embedding_model.concept_embedding.sequence_encoder.load_state_dict(seq_encoder_state)\n",
    "print(\"‚úì Sequence encoder weights loaded\")\n",
    "\n",
    "print(f\"\\nKT Embedding Model Configuration:\")\n",
    "print(f\"  ‚Ä¢ Context Model: {Config.CONTEXT_MODEL}\")\n",
    "print(f\"  ‚Ä¢ Embed Dim: {Config.EMBED_DIM}\")\n",
    "print(f\"  ‚Ä¢ Combine Method: {Config.COMBINE_METHOD}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.3 Dataset for LLM Fine-tuning\n",
    "# -----------------------------------------------------------------------------\n",
    "class KTDualEncoderDataset(Dataset):\n",
    "    \"\"\"Dataset for fine-tuning LLM with dual encoder embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, students_data, kt_embedding_model, tokenizer, \n",
    "                 question_context_emb, concept_context_emb, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.question_context_emb = question_context_emb\n",
    "        self.concept_context_emb = concept_context_emb\n",
    "        \n",
    "        # Create prompt formatter\n",
    "        self.formatter = PromptFormatter(tokenizer)\n",
    "        \n",
    "        # Flatten to individual predictions\n",
    "        self.samples = []\n",
    "        for student in students_data:\n",
    "            questions = student['questions']\n",
    "            concepts = student['concepts']\n",
    "            correctness = student['correctness']\n",
    "            question_texts = student['question_texts']\n",
    "            concept_texts = student['concept_texts']\n",
    "            \n",
    "            for t in range(1, len(questions)):\n",
    "                self.samples.append({\n",
    "                    'question_id': questions[t],\n",
    "                    'concept_id': concepts[t],\n",
    "                    'question_text': question_texts[t][:200],\n",
    "                    'concept_text': concept_texts[t],\n",
    "                    'correctness': correctness[t],\n",
    "                    'history_questions': questions[:t],\n",
    "                    'history_concepts': concepts[:t],\n",
    "                    'history_correctness': correctness[:t],\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Get context embeddings (pre-computed)\n",
    "        qid = sample['question_id']\n",
    "        cid = sample['concept_id']\n",
    "        \n",
    "        question_ctx_emb = torch.FloatTensor(\n",
    "            self.question_context_emb.get(qid, np.zeros(Config.EMBED_DIM))\n",
    "        )\n",
    "        concept_ctx_emb = torch.FloatTensor(\n",
    "            self.concept_context_emb.get(cid, np.zeros(Config.EMBED_DIM))\n",
    "        )\n",
    "        \n",
    "        # Prepare history for sequence encoder\n",
    "        history_len = len(sample['history_questions'])\n",
    "        max_hist = Config.MAX_SEQ_LEN\n",
    "        pad_len = max(0, max_hist - history_len)\n",
    "        \n",
    "        hist_questions = sample['history_questions'][-max_hist:] + [0] * pad_len\n",
    "        hist_concepts = sample['history_concepts'][-max_hist:] + [0] * pad_len\n",
    "        hist_correctness = sample['history_correctness'][-max_hist:] + [0] * pad_len\n",
    "        hist_mask = [1] * min(history_len, max_hist) + [0] * pad_len\n",
    "        \n",
    "        # Format prompt using our PromptFormatter\n",
    "        prompt = self.formatter.format_prediction_prompt(\n",
    "            question_id=qid,\n",
    "            concept_name=sample['concept_text'],\n",
    "            difficulty=\"medium\",\n",
    "            content=sample['question_text'][:100]\n",
    "        )\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'question_ctx_emb': question_ctx_emb,\n",
    "            'concept_ctx_emb': concept_ctx_emb,\n",
    "            'history_questions': torch.LongTensor(hist_questions),\n",
    "            'history_concepts': torch.LongTensor(hist_concepts),\n",
    "            'history_correctness': torch.FloatTensor(hist_correctness),\n",
    "            'history_mask': torch.FloatTensor(hist_mask),\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'label': torch.LongTensor([1 if sample['correctness'] else 0]).squeeze(0),\n",
    "            'question_id': qid,\n",
    "            'concept_id': cid\n",
    "        }\n",
    "\n",
    "print(\"\\n‚úì KTDualEncoderDataset class defined\")\n",
    "print(f\"\\nUsing LLM: {Config.LLM_MODEL_NAME}\")\n",
    "print(f\"Embedding: {Config.EMBED_DIM} ‚Üí {Config.LLM_HIDDEN_SIZE}\")\n",
    "\n",
    "# Display model info table\n",
    "print(f\"\"\"\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                  LLM MODEL CONFIGURATION                    ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  LLM Model:        {Config.LLM_MODEL_NAME:<38} ‚îÇ\n",
    "‚îÇ  LLM Hidden Size:  {Config.LLM_HIDDEN_SIZE:<38} ‚îÇ\n",
    "‚îÇ  Embed Dim:        {Config.EMBED_DIM:<38} ‚îÇ\n",
    "‚îÇ  LoRA r:           {Config.LORA_R:<38} ‚îÇ\n",
    "‚îÇ  LoRA alpha:       {Config.LORA_ALPHA:<38} ‚îÇ\n",
    "‚îÇ  Soft Tokens:      {Config.NUM_SOFT_TOKENS:<38} ‚îÇ\n",
    "‚îÇ  Combine Method:   {Config.COMBINE_METHOD:<38} ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a175b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 5.2: FINE-TUNE LLM WITH AKT EMBEDDINGS\n",
      "======================================================================\n",
      "\n",
      "[5.2.1] Initializing LLM Model...\n",
      "\n",
      "[5.1] Loading LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  ‚Üí AKT Embedding Dim: 768\n",
      "  ‚Üí LLM Hidden Size: 2048\n",
      "  ‚Üí Soft Tokens: 4\n",
      "  ‚Üí Loading with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [05:40<00:00,  1.69s/it, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí LoRA applied: r=8, alpha=16\n",
      "  ‚Üí Embedding adapter initialized\n",
      "  ‚Üí Classifier head initialized\n",
      "\n",
      "[5.2.2] Preparing Datasets...\n",
      "  ‚Üí Train samples: 73\n",
      "  ‚Üí Val samples: 24\n",
      "\n",
      "[5.2.3] Initializing WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>trainer/global_step</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val_acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_loss</td><td>‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>trainer/global_step</td><td>4</td></tr><tr><td>val_acc</td><td>0.91304</td></tr><tr><td>val_loss</td><td>0.47266</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AKT-small</strong> at: <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/zrnrugd3' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/zrnrugd3</a><br> View project at: <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260204_153653-zrnrugd3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\wandb\\run-20260204_154311-hi0c6r9g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/hi0c6r9g' target=\"_blank\">llm-small-20260204_1543</a></strong> to <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/hi0c6r9g' target=\"_blank\">https://wandb.ai/letrongducanh456-viettel/LLM-KT/runs/hi0c6r9g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[5.2.4] Starting LLM Fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.2: LLM FINE-TUNING WITH DUAL ENCODER\n",
    "# ============================================================================\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5.2: FINE-TUNE LLM WITH DUAL ENCODER EMBEDDINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2.1 Initialize the KnowledgeTracingLLM model\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2.1] Initializing LLM Model...\")\n",
    "\n",
    "kt_llm = KnowledgeTracingLLM(\n",
    "    model_name=Config.LLM_MODEL_NAME,\n",
    "    embed_dim=Config.EMBED_DIM,\n",
    "    llm_hidden_size=Config.LLM_HIDDEN_SIZE,\n",
    "    num_soft_tokens=Config.NUM_SOFT_TOKENS,\n",
    "    lora_r=Config.LORA_R,\n",
    "    lora_alpha=Config.LORA_ALPHA,\n",
    "    lora_dropout=Config.LORA_DROPOUT\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2.2 Prepare datasets\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2.2] Preparing Datasets...\")\n",
    "\n",
    "train_dataset = KTDualEncoderDataset(\n",
    "    students_data=train_data,\n",
    "    kt_embedding_model=kt_embedding_model,\n",
    "    tokenizer=kt_llm.tokenizer,\n",
    "    question_context_emb=question_context_embeddings,\n",
    "    concept_context_emb=concept_context_embeddings,\n",
    "    max_length=Config.MAX_PROMPT_LENGTH\n",
    ")\n",
    "\n",
    "val_dataset = KTDualEncoderDataset(\n",
    "    students_data=val_data,\n",
    "    kt_embedding_model=kt_embedding_model,\n",
    "    tokenizer=kt_llm.tokenizer,\n",
    "    question_context_emb=question_context_embeddings,\n",
    "    concept_context_emb=concept_context_embeddings,\n",
    "    max_length=Config.MAX_PROMPT_LENGTH\n",
    ")\n",
    "\n",
    "print(f\"  ‚Üí Train samples: {len(train_dataset):,}\")\n",
    "print(f\"  ‚Üí Val samples: {len(val_dataset):,}\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.LLM_BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.LLM_BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2.3 Initialize WandB for LLM training\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2.3] Initializing WandB...\")\n",
    "\n",
    "if Config.USE_WANDB:\n",
    "    wandb.init(\n",
    "        project=Config.WANDB_PROJECT,\n",
    "        entity=Config.WANDB_ENTITY,\n",
    "        name=f\"llm-{Config.ACTIVE_PRESET}-{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        config={\n",
    "            \"preset\": Config.ACTIVE_PRESET,\n",
    "            \"llm_model\": Config.LLM_MODEL_NAME,\n",
    "            \"llm_hidden_size\": Config.LLM_HIDDEN_SIZE,\n",
    "            \"embed_dim\": Config.EMBED_DIM,\n",
    "            \"context_model\": Config.CONTEXT_MODEL,\n",
    "            \"num_soft_tokens\": Config.NUM_SOFT_TOKENS,\n",
    "            \"combine_method\": Config.COMBINE_METHOD,\n",
    "            \"lora_r\": Config.LORA_R,\n",
    "            \"lora_alpha\": Config.LORA_ALPHA,\n",
    "            \"llm_batch_size\": Config.LLM_BATCH_SIZE,\n",
    "            \"llm_epochs\": Config.LLM_EPOCHS,\n",
    "            \"llm_lr\": Config.LLM_LR,\n",
    "            \"train_samples\": len(train_dataset),\n",
    "            \"val_samples\": len(val_dataset)\n",
    "        }\n",
    "    )\n",
    "    print(f\"‚úì WandB initialized: {wandb.run.name}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WandB disabled, logging locally only\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2.4 Training setup\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2.4] Setting up optimizer and loss...\")\n",
    "\n",
    "# Optimizer - Train adapter + classifier + LoRA weights\n",
    "llm_optimizer = optim.AdamW([\n",
    "    {'params': kt_llm.embedding_adapter.parameters(), 'lr': Config.LLM_LR},\n",
    "    {'params': kt_llm.classifier.parameters(), 'lr': Config.LLM_LR},\n",
    "    {'params': kt_llm.llm.parameters(), 'lr': Config.LLM_LR * 0.1}  # Lower LR for LoRA\n",
    "], weight_decay=0.01)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "kt_llm = kt_llm.to(device)\n",
    "kt_embedding_model = kt_embedding_model.to(device)\n",
    "\n",
    "print(f\"  ‚Üí Device: {device}\")\n",
    "print(f\"  ‚Üí Learning rate: {Config.LLM_LR}\")\n",
    "print(f\"  ‚Üí Epochs: {Config.LLM_EPOCHS}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5.2.5 Training loop\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[5.2.5] Starting LLM Fine-tuning...\")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "for epoch in range(Config.LLM_EPOCHS):\n",
    "    # Training phase\n",
    "    kt_llm.train()\n",
    "    kt_embedding_model.eval()  # Keep embedding model in eval mode\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{Config.LLM_EPOCHS} [Train]\")\n",
    "    for batch in progress_bar:\n",
    "        # Get context embeddings\n",
    "        question_ctx_emb = batch['question_ctx_emb'].to(device)\n",
    "        concept_ctx_emb = batch['concept_ctx_emb'].to(device)\n",
    "        \n",
    "        # Get sequence embeddings from history\n",
    "        hist_questions = batch['history_questions'].to(device)\n",
    "        hist_concepts = batch['history_concepts'].to(device)\n",
    "        hist_correctness = batch['history_correctness'].to(device)\n",
    "        hist_mask = batch['history_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get sequence embeddings\n",
    "            seq_emb = kt_embedding_model.question_embedding.sequence_encoder(\n",
    "                hist_questions, hist_concepts, hist_correctness, hist_mask\n",
    "            )\n",
    "            # Get last valid position\n",
    "            batch_size = seq_emb.size(0)\n",
    "            seq_lengths = hist_mask.sum(dim=1).long()\n",
    "            seq_emb_final = torch.stack([\n",
    "                seq_emb[i, max(0, seq_lengths[i].item() - 1), :] \n",
    "                for i in range(batch_size)\n",
    "            ])\n",
    "        \n",
    "        # Combine context + sequence embeddings\n",
    "        if Config.COMBINE_METHOD == 'add':\n",
    "            combined_emb = question_ctx_emb + seq_emb_final\n",
    "        elif Config.COMBINE_METHOD == 'concat':\n",
    "            combined_emb = torch.cat([question_ctx_emb, seq_emb_final], dim=-1)\n",
    "        else:  # mean\n",
    "            combined_emb = (question_ctx_emb + seq_emb_final) / 2\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        llm_optimizer.zero_grad()\n",
    "        logits = kt_llm(combined_emb, input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(kt_llm.parameters(), 1.0)\n",
    "        llm_optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        epoch_correct += (preds == labels).sum().item()\n",
    "        epoch_total += labels.size(0)\n",
    "        \n",
    "        progress_bar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{epoch_correct/epoch_total:.3f}\")\n",
    "    \n",
    "    train_loss = epoch_loss / len(train_loader)\n",
    "    train_acc = epoch_correct / epoch_total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation phase\n",
    "    kt_llm.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{Config.LLM_EPOCHS} [Val]\"):\n",
    "            question_ctx_emb = batch['question_ctx_emb'].to(device)\n",
    "            hist_questions = batch['history_questions'].to(device)\n",
    "            hist_concepts = batch['history_concepts'].to(device)\n",
    "            hist_correctness = batch['history_correctness'].to(device)\n",
    "            hist_mask = batch['history_mask'].to(device)\n",
    "            \n",
    "            seq_emb = kt_embedding_model.question_embedding.sequence_encoder(\n",
    "                hist_questions, hist_concepts, hist_correctness, hist_mask\n",
    "            )\n",
    "            batch_size = seq_emb.size(0)\n",
    "            seq_lengths = hist_mask.sum(dim=1).long()\n",
    "            seq_emb_final = torch.stack([\n",
    "                seq_emb[i, max(0, seq_lengths[i].item() - 1), :] \n",
    "                for i in range(batch_size)\n",
    "            ])\n",
    "            \n",
    "            if Config.COMBINE_METHOD == 'add':\n",
    "                combined_emb = question_ctx_emb + seq_emb_final\n",
    "            else:\n",
    "                combined_emb = (question_ctx_emb + seq_emb_final) / 2\n",
    "            \n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            logits = kt_llm(combined_emb, input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            val_correct += (preds == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader)\n",
    "    val_acc = val_correct / val_total\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Log to WandB\n",
    "    if Config.USE_WANDB:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_acc\": val_acc\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{Config.LLM_EPOCHS}:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': kt_llm.state_dict(),\n",
    "            'optimizer_state_dict': llm_optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_loss': val_loss\n",
    "        }, f'{Config.LLM_CHECKPOINT_DIR}/best_kt_llm.pt')\n",
    "        print(f\"  ‚úì New best model saved! Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úì LLM FINE-TUNING COMPLETE\")\n",
    "print(f\"  ‚Üí Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b627e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training function defined: train_kt_llama()\n",
      "======================================================================\n",
      "\n",
      "Training Configuration:\n",
      "  - Optimizer: AdamW with weight decay\n",
      "  - Scheduler: CosineAnnealingLR\n",
      "  - Loss: CrossEntropyLoss\n",
      "  - Gradient Clipping: max_norm=1.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5.3: PLOT LLM TRAINING CURVES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 5.3: VISUALIZE LLM TRAINING PROGRESS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs_range = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(epochs_range, train_losses, 'b-', linewidth=2, label='Train Loss', marker='o', markersize=4)\n",
    "axes[0].plot(epochs_range, val_losses, 'r-', linewidth=2, label='Val Loss', marker='s', markersize=4)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('LLM Fine-tuning: Loss Curves', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xticks(epochs_range)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(epochs_range, train_accs, 'b-', linewidth=2, label='Train Accuracy', marker='o', markersize=4)\n",
    "axes[1].plot(epochs_range, val_accs, 'r-', linewidth=2, label='Val Accuracy', marker='s', markersize=4)\n",
    "axes[1].axhline(y=best_val_acc, color='g', linestyle='--', alpha=0.7, label=f'Best Val Acc: {best_val_acc:.3f}')\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1].set_title('LLM Fine-tuning: Accuracy Curves', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xticks(epochs_range)\n",
    "\n",
    "plt.suptitle(f'LLM Training Summary ({Config.ACTIVE_PRESET} preset)', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'llm_training_curves.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Training curves saved to: {os.path.join(Config.OUTPUT_DIR, 'llm_training_curves.png')}\")\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"  ‚Üí Final Train Loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  ‚Üí Final Val Loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"  ‚Üí Final Train Acc: {train_accs[-1]:.4f}\")\n",
    "print(f\"  ‚Üí Final Val Acc: {val_accs[-1]:.4f}\")\n",
    "print(f\"  ‚Üí Best Val Acc: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49535482",
   "metadata": {},
   "source": [
    "## üìä Step 6: Evaluation & Testing\n",
    "\n",
    "**Objective:** Load the fine-tuned model and test with sample predictions.\n",
    "\n",
    "**Tasks:**\n",
    "1. Load the best checkpoint\n",
    "2. Test on sample student-question pairs\n",
    "3. Compute evaluation metrics (Accuracy, AUC, F1)\n",
    "4. Visualize confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4605c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STARTING TINYLLAMA-1.1B FINE-TUNING FOR KNOWLEDGE TRACING\n",
      "======================================================================\n",
      "\n",
      "[1/6] Loading AKT embeddings and enriched metadata...\n",
      "‚úì Loaded embeddings - Train: (8, 4096), Val: (2, 4096)\n",
      "‚úì Metadata split - Train: 8 students, Val: 2 students\n",
      "\n",
      "[2/6] Initializing TinyLlama-1.1B model...\n",
      "  ‚Üí Loading model with 4-bit quantization...\n",
      "  ‚Üí This may take a few minutes on first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "'[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ‚Üí Loading model with 4-bit quantization...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  ‚Üí This may take a few minutes on first run...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m kt_llama = \u001b[43mKnowledgeTracingLlama\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_soft_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\n\u001b[32m     35\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Model initialized successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ‚Üí Trainable parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mkt_llama.parameters()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mp.requires_grad)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mKnowledgeTracingLlama.__init__\u001b[39m\u001b[34m(self, model_name, embed_dim, num_soft_tokens)\u001b[39m\n\u001b[32m     19\u001b[39m bnb_config = BitsAndBytesConfig(\n\u001b[32m     20\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     21\u001b[39m     bnb_4bit_use_double_quant=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     22\u001b[39m     bnb_4bit_quant_type=\u001b[33m\"\u001b[39m\u001b[33mnf4\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m     bnb_4bit_compute_dtype=torch.float16\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Load Llama2-7B tokenizer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer.pad_token = \u001b[38;5;28mself\u001b[39m.tokenizer.eos_token\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Load Llama2-7B model with quantization\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:621\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m    617\u001b[39m         config = AutoConfig.from_pretrained(\n\u001b[32m    618\u001b[39m             pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **kwargs\n\u001b[32m    619\u001b[39m         )\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m621\u001b[39m         config = \u001b[43mPreTrainedConfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    623\u001b[39m config_model_type = config.model_type\n\u001b[32m    625\u001b[39m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:531\u001b[39m, in \u001b[36mPreTrainedConfig.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mlocal_files_only\u001b[39m\u001b[33m\"\u001b[39m] = local_files_only\n\u001b[32m    529\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m] = revision\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.base_config_key \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.base_config_key \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[32m    533\u001b[39m     config_dict = config_dict[\u001b[38;5;28mcls\u001b[39m.base_config_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:569\u001b[39m, in \u001b[36mPreTrainedConfig.get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    567\u001b[39m original_kwargs = copy.deepcopy(kwargs)\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m config_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\configuration_utils.py:624\u001b[39m, in \u001b[36mPreTrainedConfig._get_config_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    620\u001b[39m configuration_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33m_configuration_file\u001b[39m\u001b[33m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    623\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    637\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    638\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:276\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    222\u001b[39m     path_or_repo_id: \u001b[38;5;28mstr\u001b[39m | os.PathLike,\n\u001b[32m    223\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    224\u001b[39m     **kwargs,\n\u001b[32m    225\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    226\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    228\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    274\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:419\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    418\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    420\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    422\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    433\u001b[39m         snapshot_download(\n\u001b[32m    434\u001b[39m             path_or_repo_id,\n\u001b[32m    435\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    443\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    444\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1024\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, etag_timeout, token, local_files_only, headers, endpoint, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1003\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m   1004\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m   1005\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1021\u001b[39m         dry_run=dry_run,\n\u001b[32m   1022\u001b[39m     )\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1157\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, force_download, tqdm_class, dry_run)\u001b[39m\n\u001b[32m   1151\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, _DEFAULT_RETRY_ON_EXCEPTIONS) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1152\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError)\n\u001b[32m   1153\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code \u001b[38;5;129;01min\u001b[39;00m _DEFAULT_RETRY_ON_STATUS_CODES\n\u001b[32m   1154\u001b[39m     ):\n\u001b[32m   1155\u001b[39m         logger.info(\u001b[33m\"\u001b[39m\u001b[33mNo local file found. Retrying..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1156\u001b[39m         (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = (\n\u001b[32m-> \u001b[39m\u001b[32m1157\u001b[39m             \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_ETAG_RETRY_TIMEOUT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1171\u001b[39m         )\n\u001b[32m   1173\u001b[39m \u001b[38;5;66;03m# If still error, raise\u001b[39;00m\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1691\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder, retry_on_errors)\u001b[39m\n\u001b[32m   1689\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1690\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1691\u001b[39m         metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1696\u001b[39m \u001b[43m            \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1697\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m RemoteEntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[32m   1700\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1701\u001b[39m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:89\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     85\u001b[39m         validate_repo_id(arg_value)\n\u001b[32m     87\u001b[39m kwargs = smoothly_deprecate_legacy_arguments(fn_name=fn.\u001b[34m__name__\u001b[39m, kwargs=kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1614\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, timeout, library_name, library_version, user_agent, headers, endpoint, retry_on_errors)\u001b[39m\n\u001b[32m   1611\u001b[39m hf_headers[\u001b[33m\"\u001b[39m\u001b[33mAccept-Encoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33midentity\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[32m   1613\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1614\u001b[39m response = \u001b[43m_httpx_follow_relative_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1615\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_on_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_errors\u001b[49m\n\u001b[32m   1616\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1617\u001b[39m hf_raise_for_status(response)\n\u001b[32m   1619\u001b[39m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:302\u001b[39m, in \u001b[36m_httpx_follow_relative_redirects\u001b[39m\u001b[34m(method, url, retry_on_errors, **httpx_kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m no_retry_kwargs: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = (\n\u001b[32m    298\u001b[39m     {} \u001b[38;5;28;01mif\u001b[39;00m retry_on_errors \u001b[38;5;28;01melse\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mretry_on_exceptions\u001b[39m\u001b[33m\"\u001b[39m: (), \u001b[33m\"\u001b[39m\u001b[33mretry_on_status_codes\u001b[39m\u001b[33m\"\u001b[39m: ()}\n\u001b[32m    299\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     response = \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhttpx_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mno_retry_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m     hf_raise_for_status(response)\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# Check if response is a relative redirect\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:506\u001b[39m, in \u001b[36mhttp_backoff\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_backoff\u001b[39m(\n\u001b[32m    442\u001b[39m     method: HTTP_METHOD_T,\n\u001b[32m    443\u001b[39m     url: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m     **kwargs,\n\u001b[32m    451\u001b[39m ) -> httpx.Response:\n\u001b[32m    452\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wrapper around httpx to retry calls on an endpoint, with exponential backoff.\u001b[39;00m\n\u001b[32m    453\u001b[39m \n\u001b[32m    454\u001b[39m \u001b[33;03m    Endpoint call is retried on exceptions (ex: connection timeout, proxy error,...)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    504\u001b[39m \u001b[33;03m    > issue on [Github](https://github.com/huggingface/huggingface_hub).\u001b[39;00m\n\u001b[32m    505\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_http_backoff_base\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_wait_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m            \u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_on_status_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:414\u001b[39m, in \u001b[36m_http_backoff_base\u001b[39m\u001b[34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, stream, **kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_retry(response):\n\u001b[32m    416\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:101\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:78\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:124\u001b[39m, in \u001b[36mHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_backend\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_tcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\DUCANH\\OneDrive - VNU-HCMUS\\Desktop\\AI tutor\\LLM-KT\\.venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:208\u001b[39m, in \u001b[36mSyncBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    202\u001b[39m exc_map: ExceptionMapping = {\n\u001b[32m    203\u001b[39m     socket.timeout: ConnectTimeout,\n\u001b[32m    204\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    205\u001b[39m }\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     sock = \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n\u001b[32m    214\u001b[39m         sock.setsockopt(*option)  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:844\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, all_errors)\u001b[39m\n\u001b[32m    842\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m error \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    843\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[32m--> \u001b[39m\u001b[32m844\u001b[39m         \u001b[43mexceptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclear\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# raise only the last error\u001b[39;00m\n\u001b[32m    845\u001b[39m     exceptions.append(exc)\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: EVALUATION & TESTING\n",
    "# ============================================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 6: EVALUATE FINE-TUNED LLM WITH DUAL ENCODER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load best checkpoint\n",
    "print(\"\\n[6.1] Loading Best Model Checkpoint...\")\n",
    "checkpoint_path = f'{Config.LLM_CHECKPOINT_DIR}/best_kt_llm.pt'\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    kt_llm.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"  ‚Üí Loaded checkpoint from epoch {checkpoint['epoch']+1}\")\n",
    "    print(f\"  ‚Üí Val Accuracy: {checkpoint['val_acc']:.4f}\")\n",
    "else:\n",
    "    print(\"  ‚ö† No checkpoint found, using current model state\")\n",
    "\n",
    "kt_llm.eval()\n",
    "kt_embedding_model.eval()\n",
    "\n",
    "# Full evaluation on validation set\n",
    "print(\"\\n[6.2] Running Full Evaluation...\")\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "        # Get context embeddings\n",
    "        question_ctx_emb = batch['question_ctx_emb'].to(device)\n",
    "        \n",
    "        # Get sequence embeddings from history\n",
    "        hist_questions = batch['history_questions'].to(device)\n",
    "        hist_concepts = batch['history_concepts'].to(device)\n",
    "        hist_correctness = batch['history_correctness'].to(device)\n",
    "        hist_mask = batch['history_mask'].to(device)\n",
    "        \n",
    "        # Get sequence embeddings\n",
    "        seq_emb = kt_embedding_model.question_embedding.sequence_encoder(\n",
    "            hist_questions, hist_concepts, hist_correctness, hist_mask\n",
    "        )\n",
    "        batch_size = seq_emb.size(0)\n",
    "        seq_lengths = hist_mask.sum(dim=1).long()\n",
    "        seq_emb_final = torch.stack([\n",
    "            seq_emb[i, max(0, seq_lengths[i].item() - 1), :] \n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "        \n",
    "        # Combine context + sequence\n",
    "        if Config.COMBINE_METHOD == 'add':\n",
    "            combined_emb = question_ctx_emb + seq_emb_final\n",
    "        else:\n",
    "            combined_emb = (question_ctx_emb + seq_emb_final) / 2\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        logits = kt_llm(combined_emb, input_ids, attention_mask)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        preds = torch.argmax(probs, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "# Compute metrics\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "try:\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "except:\n",
    "    auc = 0.0\n",
    "f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  AUC:       {auc:.4f}\")\n",
    "print(f\"  F1 Score:  {f1:.4f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n[6.3] Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=['Incorrect', 'Correct']))\n",
    "\n",
    "# Confusion Matrix\n",
    "print(\"\\n[6.4] Confusion Matrix:\")\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Incorrect', 'Correct'], \n",
    "            yticklabels=['Incorrect', 'Correct'], ax=axes[0])\n",
    "axes[0].set_xlabel('Predicted', fontsize=12)\n",
    "axes[0].set_ylabel('Actual', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix', fontsize=14)\n",
    "\n",
    "# Probability Distribution\n",
    "axes[1].hist(all_probs[all_labels == 0], bins=30, alpha=0.6, label='Incorrect', color='red')\n",
    "axes[1].hist(all_probs[all_labels == 1], bins=30, alpha=0.6, label='Correct', color='green')\n",
    "axes[1].set_xlabel('Predicted Probability (Correct)', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Prediction Probability Distribution', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "plt.savefig(os.path.join(Config.OUTPUT_DIR, 'evaluation_results.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Test with Sample Predictions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n[6.5] Sample Predictions:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "num_samples = 5\n",
    "sample_indices = np.random.choice(len(val_dataset), num_samples, replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    sample = val_dataset[idx]\n",
    "    \n",
    "    # Get embeddings for sample\n",
    "    question_ctx_emb = sample['question_ctx_emb'].unsqueeze(0).to(device)\n",
    "    hist_questions = sample['history_questions'].unsqueeze(0).to(device)\n",
    "    hist_concepts = sample['history_concepts'].unsqueeze(0).to(device)\n",
    "    hist_correctness = sample['history_correctness'].unsqueeze(0).to(device)\n",
    "    hist_mask = sample['history_mask'].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        seq_emb = kt_embedding_model.question_embedding.sequence_encoder(\n",
    "            hist_questions, hist_concepts, hist_correctness, hist_mask\n",
    "        )\n",
    "        seq_emb_final = seq_emb[0, max(0, int(hist_mask.sum().item()) - 1), :]\n",
    "        \n",
    "        if Config.COMBINE_METHOD == 'add':\n",
    "            combined_emb = question_ctx_emb + seq_emb_final.unsqueeze(0)\n",
    "        else:\n",
    "            combined_emb = (question_ctx_emb + seq_emb_final.unsqueeze(0)) / 2\n",
    "    \n",
    "    input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "    attention_mask = sample['attention_mask'].unsqueeze(0).to(device)\n",
    "    true_label = sample['label'].item()\n",
    "    question_id = sample['question_id']\n",
    "    concept_id = sample['concept_id']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = kt_llm(combined_emb, input_ids, attention_mask)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        pred = torch.argmax(probs, dim=-1).item()\n",
    "        confidence = probs[0, pred].item()\n",
    "    \n",
    "    true_str = \"Correct\" if true_label == 1 else \"Incorrect\"\n",
    "    pred_str = \"Correct\" if pred == 1 else \"Incorrect\"\n",
    "    match = \"‚úì\" if pred == true_label else \"‚úó\"\n",
    "    \n",
    "    print(f\"\\nSample {i+1}: Q={question_id}, C={concept_id}\")\n",
    "    print(f\"  True: {true_str:10} | Predicted: {pred_str:10} | Confidence: {confidence:.3f} | {match}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Evaluation Complete!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SUMMARY & NEXT STEPS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üéâ DUAL ENCODER + LLM PIPELINE COMPLETE!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                        EXPERIMENT SUMMARY                             ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë  Preset Used:     {Config.ACTIVE_PRESET:49} ‚ïë\n",
    "‚ïë  LLM Model:       {Config.LLM_MODEL_NAME[:49]:49} ‚ïë\n",
    "‚ïë  Context Model:   {Config.CONTEXT_MODEL[:49]:49} ‚ïë\n",
    "‚ïë  Embed Dim:       {str(Config.EMBED_DIM):49} ‚ïë\n",
    "‚ïë  Combine Method:  {Config.COMBINE_METHOD:49} ‚ïë\n",
    "‚ïë  Best Val Acc:    {str(round(best_val_acc, 4)):49} ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")\n",
    "\n",
    "# Verify saved files\n",
    "print(\"üìÅ Saved Checkpoints & Outputs:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "saved_files = [\n",
    "    (\"Sequence Encoder\", f\"{Config.AKT_CHECKPOINT_DIR}/*.ckpt\"),\n",
    "    (\"LLM Best Model\", f\"{Config.LLM_CHECKPOINT_DIR}/best_kt_llm.pt\"),\n",
    "    (\"Concept Mappings\", f\"{Config.MAPPINGS_DIR}/concept_mapping.json\"),\n",
    "    (\"Question Mappings\", f\"{Config.MAPPINGS_DIR}/question_mapping.json\"),\n",
    "    (\"Train Seq Embeddings\", Config.TRAIN_EMBEDDINGS),\n",
    "    (\"Val Seq Embeddings\", Config.VAL_EMBEDDINGS),\n",
    "    (\"Question Context Emb\", \"cache/embeddings/question_context.npy\"),\n",
    "    (\"Concept Context Emb\", \"cache/embeddings/concept_context.npy\"),\n",
    "    (\"Training Curves\", f\"{Config.OUTPUT_DIR}/llm_training_curves.png\"),\n",
    "    (\"Evaluation Plot\", f\"{Config.OUTPUT_DIR}/evaluation_results.png\"),\n",
    "    (\"Dataset Analysis\", \"assets/dataset_analysis.png\"),\n",
    "]\n",
    "\n",
    "import glob\n",
    "\n",
    "for name, path in saved_files:\n",
    "    if '*' in path:\n",
    "        matches = glob.glob(path)\n",
    "        if matches:\n",
    "            print(f\"  ‚úì {name:25} ‚Üí {matches[0]}\")\n",
    "        else:\n",
    "            print(f\"  ‚úó {name:25} ‚Üí Not found\")\n",
    "    elif os.path.exists(path):\n",
    "        file_size = os.path.getsize(path)\n",
    "        size_str = f\"{file_size / (1024 * 1024):.2f} MB\" if file_size > 1024*1024 else f\"{file_size / 1024:.1f} KB\"\n",
    "        print(f\"  ‚úì {name:25} ‚Üí {path} ({size_str})\")\n",
    "    else:\n",
    "        print(f\"  ‚úó {name:25} ‚Üí {path} (not found)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "# Architecture summary\n",
    "print(\"\\nüìê Architecture Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "  DUAL ENCODER SYSTEM:\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ  Context Encoder: {Config.CONTEXT_MODEL:38} ‚îÇ\n",
    "  ‚îÇ  Sequence Encoder: Transformer ({Config.NUM_SEQ_LAYERS} layers, {Config.NUM_HEADS} heads){' '*12} ‚îÇ\n",
    "  ‚îÇ  Combine Method: {Config.COMBINE_METHOD:40} ‚îÇ\n",
    "  ‚îÇ  Embedding Formula: Question = Context(text) + Seq(history)‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "  \n",
    "  LLM FINE-TUNING:\n",
    "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "  ‚îÇ  LLM: {Config.LLM_MODEL_NAME:51} ‚îÇ\n",
    "  ‚îÇ  LoRA r={Config.LORA_R}, alpha={Config.LORA_ALPHA}{' '*39} ‚îÇ\n",
    "  ‚îÇ  Soft Tokens: {Config.NUM_SOFT_TOKENS}{' '*43} ‚îÇ\n",
    "  ‚îÇ  Prediction: Yes/No (binary classification){' '*14} ‚îÇ\n",
    "  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\"\"\")\n",
    "\n",
    "# CLI usage\n",
    "print(\"\\nüñ•Ô∏è CLI Usage:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\"\"\n",
    "  TRAINING:\n",
    "    python train.py --preset {Config.ACTIVE_PRESET} --epochs {Config.LLM_EPOCHS}\n",
    "    python train.py --preset qwen --epochs 10 --wandb\n",
    "  \n",
    "  TESTING:\n",
    "    python test.py --checkpoint {Config.LLM_CHECKPOINT_DIR}/best_kt_llm.pt\n",
    "    python test.py --checkpoint checkpoints/best.pt --output results/\n",
    "\"\"\")\n",
    "\n",
    "# Directory summary\n",
    "print(\"\\nüìÇ Directory Structure:\")\n",
    "print(\"=\" * 70)\n",
    "directories = [\n",
    "    Config.AKT_CHECKPOINT_DIR,\n",
    "    Config.LLM_CHECKPOINT_DIR,\n",
    "    Config.EMBEDDING_DIR,\n",
    "    Config.MAPPINGS_DIR,\n",
    "    Config.OUTPUT_DIR,\n",
    "    Config.AKT_LOG_DIR,\n",
    "    \"cache/embeddings\",\n",
    "    \"assets\"\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    if os.path.exists(directory):\n",
    "        file_count = len([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))])\n",
    "        print(f\"  üìÅ {directory:30} ‚Üí {file_count} files\")\n",
    "    else:\n",
    "        print(f\"  üìÅ {directory:30} ‚Üí (not created)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "\n",
    "print(\"\\nüîÑ To try a different LLM preset:\")\n",
    "print(\"  1. Change PRESET variable in Cell 2\")\n",
    "print(\"  2. Re-run from Cell 2 onwards\")\n",
    "print(\"  3. Available presets: small, standard, phi3, qwen, llama2\")\n",
    "\n",
    "print(\"\\nüìù Quick Reference:\")\n",
    "Config.list_presets()\n",
    "\n",
    "print(\"\\n‚úÖ All done! Check WandB for training logs and comparisons.\")\n",
    "if Config.USE_WANDB:\n",
    "    print(f\"   WandB Project: {Config.WANDB_PROJECT}\")\n",
    "    print(f\"   WandB Entity: {Config.WANDB_ENTITY}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
